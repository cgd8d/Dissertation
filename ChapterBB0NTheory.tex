\renewcommand{\thechapter}{2}
\chapter{Theory of Neutrinoless Double-Beta Decay}
\label{ch:BB0NTheory}

Toward the end of the 1990s neutrinos were conclusively proven to possess a non-zero mass.  Subsequent effort has been directed toward understanding the absolute scale of neutrino mass spectrum and the nature of the interaction which causes it.  Neutrinoless double-beta ($\beta\beta 0\nu$) decay, would provide a window onto both questions.  This chapter presents the theoretical motivations for searching for $\beta\beta 0\nu$ decay.  Section~\ref{sec:TheoryStandardDoubleBeta} describes the closely-related two-neutrino double-beta ($\beta\beta 2\nu$) decay which is allowed in the standard model.  Section~\ref{sec:TheoryNeutrinolessDoubleBeta} defines $\beta\beta 0\nu$ decay and reasons for hypothesizing its occurrence.  Considerations and challenges in nuclear physics are described in section~\ref{sec:NuclearMatrixCalculations}.  The parametrization of the neutrino sector is provided in section~\ref{sec:NeutrinoFlavorPhysics}.  Section \ref{sec:ParticlePhysicsConstraints} summarizes constraints on neutrino masses which can be obtained from observations of single-$\beta$ decay and cosmology, and section \ref{sec:NucPhysConstraintsFromBB0N} describes in detail the considerations and challenges involved in searching for $\beta\beta 0\nu$ decay.  The reader will obtain a general understanding of the state of the field before continuing on to the details of the EXO-200 detector in chapter~\ref{ch:EXO200Detector}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=2in]{Avignone_fig02a.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Feynman diagram for $\beta\beta 2 \nu$ decay.  The reaction products are equivalent to two $\beta$ decays in succession, but this reaction can sometimes occur even if a single $\beta$ decay would be energetically forbidden.  Figure from~\cite{RMPbb0n}.}
\label{fig:FeynmanBetaBeta2Nu}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\section{Two-Neutrino Double-Beta Decay}\label{sec:TheoryStandardDoubleBeta}

Standard-model two-neutrino double beta ($\beta\beta 2\nu$) decay is the result of the particle interaction
\begin{equation}\label{eqn:bb2n_decay_reaction}
2d \rightarrow 2u + 2e^- + 2\bar{\nu}_e
\end{equation}
mediated by $W^-$-exchange, as depicted in Figure~\ref{fig:FeynmanBetaBeta2Nu}.  It is effectively the simultaneous occurrence of two beta ($\beta$) decays from the same nucleus.

Because $\beta\beta 2\nu$ decay is a second-order weak interaction, it has a remarkably slow rate compared to most $\beta$ decay processes.  Although many nuclei are expected to decay by $\beta\beta 2\nu$, the process is thoroughly masked by conventional $\beta$ decay in most of them.  In most cases, we can only hope to detect $\beta\beta 2\nu$ decay in isotopes where $\beta$ decay is forbidden or highly suppressed.

An example of an isotope for which $\beta$ decay is highly suppressed is $^{48}_{20}$Ca.  The ground state of $^{48}_{20}$Ca has zero units of angular momentum, whereas its single-$\beta$ decay daughter product $^{48}_{21}$Sc has six units of total angular momentum in its ground state, and is thus highly suppressed by angular momentum conservation.  In contrast, $^{48}_{22}$Ti has zero units of total angular momentum, making $\beta\beta 2\nu$ decay of $^{48}_{20}$Ca permitted by angular momentum and energy considerations, and as a result the $\beta\beta 2\nu$ decay mode of $^{48}_{20}$Ca dominates~\cite{MyNuclearPhysicsBook}.

In the most promising $\beta\beta 2\nu$ candidates, single $\beta$ decay is forbidden by energy conservation.  It is well-known that nuclei minimize their energy by arranging similar nucleons to have overlapping wavefunctions~\cite{MyNuclearPhysicsBook}.  Thus, isotopes with an even number of protons and an even number of neutrons will have less nucleon-pairing potential energy than an isotope with either an odd number of protons or an odd number of neutrons, which in turn will have less nucleon-pairing potential energy than an isotope with an odd number of protons and an odd number of neutrons.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{scripts/LevelDiagram.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Energy diagram of isotopes with atomic mass $A=136$.  The energies $\Delta E$ are the binding energies of the atom, compared to the bare masses of the same nuclei.  Values are from~\cite{AtomicMassEvaluation}.}
\label{fig:LevelDiagram}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The effect is illustrated by the nuclear energy level diagram shown in figure~\ref{fig:LevelDiagram} for the $A=136$ isobar. Xenon, barium, cerium, and neodymium are even-even isotopes, and have systematically lower energies than the odd-odd isotopes iodine, cesium, lanthanum, and praseodymium.  For this particular isobar, we can see that xenon is energetically forbidden from single-$\beta$ decaying to cesium because the odd-odd isotope of cesium has slightly more potential energy than the even-even isotope of xenon. As a result, the primary mode of decay of xenon-136 will be $\beta\beta 2\nu$ decay to barium-136.  Similarly, cerium-136 can undergo double-electron capture, electron capture with positron emission, or double-positron emission; however, in practice the expected rates for these decays will be lower than the rates for $\beta\beta 2\nu$ decay, so we will not consider them further in this work.

\section{Neutrinoless Double-Beta Decay}\label{sec:TheoryNeutrinolessDoubleBeta}

The detection and study of $\beta\beta 2\nu$ decay provides an opportunity to test a class of nuclear matrix element computations; however, the decay does not violate any fundamental symmetries and its existence is, in this sense, a mundane prediction of the Standard Model.  The primary appeal of isotopes which undergo $\beta\beta 2\nu$ decay is the opportunity these isotopes provide to probe the nature of neutrinos through the related neutrinoless decay.

It had been suggested as early as 1937 that neutrinos could possess mass through a neutrino-antineutrino interaction, provided that the neutrino is its own antiparticle~\cite{Majorana}.  The theorized Majorana interaction comes from Lagrangian terms of the form (for each of three neutrino eigenstates)
\begin{align}
\mathcal{L}_{Maj}&= \begin{aligned}[t]
 & -\frac{m_{L}}{2} \left( \overline{\Psi_L^c} \Psi_L^{} + \overline{\Psi_L^{}} \Psi_L^c \right)\\
 & -\frac{m_{R}}{2} \left( \overline{\Psi_R^c} \Psi_R^{} + \overline{\Psi_R^{}} \Psi_R^c \right),
\end{aligned}\label{eqn:MajoranaLagrangianTerms}
\end{align}
where the superscript-$c$ represents charge conjugation.  (This is the reason that Majorana mass terms are only possible for a chargeless lepton.)  The masses $m_L$ and $m_R$ may be chosen independently; since there has never been an observation of right-handed neutrinos or left-handed anti-neutrinos, it is possible that $m_R = 0$ and that the fields $\Psi_R^{}$ and $\Psi_R^c$ do not exist in nature~\cite{RMPbb0n}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=2in]{Avignone_fig02b.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Feynman diagram for $\beta\beta 0 \nu$ decay.  A virtual neutrino mediates the exchange.  This is only possible if $\overline{\nu}_R$ can flip its handedness to $\nu_L$, and the interaction that induces this parity change also generates neutrino mass.  Figure from~\cite{RMPbb0n}.}
\label{fig:FeynmanBetaBeta0Nu}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

If neutrinos do have Majorana mass interactions, then any isotope that undergoes $\beta\beta 2\nu$ decay can also undergo the related process $2d \rightarrow 2u + 2e^-$, depicted in Figure~\ref{fig:FeynmanBetaBeta0Nu}, in which the two outgoing neutrinos in $\beta\beta 2\nu$ decay are replaced by one virtual neutrino.  This process is called neutrinoless double-beta ($\beta\beta 0\nu$) decay.  We can interpret this as a mixing interaction between a left-handed neutrino and a right-handed antineutrino; this is only possible if neutrinos are Majorana particles.

The tree-level diagram for $\beta\beta 0\nu$ has one additional interaction vertex compared to $\beta\beta 2\nu$ decay, and as a result we would expect it to occur at an even slower rate.  However, the more immediate consequence of $\beta\beta 0\nu$ decay is that lepton number conservation is violated.  The lepton number changes by two units corresponding to the creation of two leptons with no balancing anti-leptons.  Numerous theories have suggested other plausible modes of lepton number non-conservation~\cite{ProtonDecay,MuonToPositron}, but none have yet reported a positive result.  In the conventional Standard Model with massless neutrinos, lepton number conservation is an accidental symmetry~\cite{LeptonConservation}, but in a model with massive neutrinos there may no longer be any reason \textit{a priori} to expect conservation of lepton number.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{Avignone_fig03.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Even without any assumptions about the mechanism which leads to $\beta\beta 0\nu$ decay, we can use that process to generate an effective neutrino mass as a higher-order process.  Figure from~\cite{RMPbb0n}.}
\label{fig:FeynmanBetaBeta0NuImplication}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

It is worth noting that the interaction term above assumes no mediating particles in the mass mechanism, whereas it is possible that $\beta\beta 0\nu$ could be mediated by some higher-order interaction terms.  However, if $\beta\beta 0\nu$ decay is observed, it leads very generally to a conclusion that neutrinos have an \emph{effective} Majorana mass interaction~\cite{BlackBoxTheorem}.  We can see this by embedding the $\beta\beta 0\nu$ process into a higher-order one, as shown in figure~\ref{fig:FeynmanBetaBeta0NuImplication}.  Regardless of the details of how $\beta\beta 0\nu$ occurs, its existence would necessarily generate an effective neutrino mass as a higher-order process.

We have described so far only the theorized Majorana interaction of equation~\ref{eqn:MajoranaLagrangianTerms}.  However, it is also possible that neutrinos could have a Dirac mass term analogous to the other fermions.  The full set of neutrino mass terms in the Lagrangian would then be:
\begin{align}
\mathcal{L}_{Maj+Dirac}&= \begin{aligned}[t]
 & -\frac{m_{L}}{2} \left( \overline{\Psi_L^c} \Psi_L^{} + \overline{\Psi_L^{}} \Psi_L^c \right)\\
 & -\frac{m_{R}}{2} \left( \overline{\Psi_R^c} \Psi_R^{} + \overline{\Psi_R^{}} \Psi_R^c \right)\\
 & -m_D \left(\overline{\Psi_L}\Psi_R + \overline{\Psi_R}\Psi_L\right),
\end{aligned}
\end{align}
where $m_D$ is the new Dirac mass term.  The three flavors observed in nature would result in three sets of these terms, one for each flavor, and nine possible mass terms in total~\cite{RMPbb0n}.

For simplicity, we consider now the mass terms for a one-flavor system.   We can rearrange the Lagrangian terms as:
\begin{subequations}\begin{align}
\mathcal{L}_{Maj+Dirac}&= -\frac{1}{2} \left(\overline{\left(n_L\right)^c} \mathcal{M} n_L + \overline{\left(n_L\right)^{}} \mathcal{M} n_L^c\right), \text{where}\\
\mathcal{M}&= \begin{pmatrix}m_L & m_D \\ m_D & m_R \end{pmatrix}\\
n_L &= \begin{pmatrix} \Psi_L \\ \left(\Psi_R\right)^c \end{pmatrix}.
\end{align}\end{subequations}
The matrix $\mathcal{M}$ can, in all cases, be diagonalized by some unitary matrix, which means that there is always a basis in which two interaction terms are sufficient.  In nearly all cases the eigenvalues will be distinct values, so the two eigenstates are non-degenerate.  Both eigenstates in this case are their own antiparticles, so they are Majorana.  The only exceptions to this are when $m_L = m_R$ and $m_D = 0$, or when $m_L = m_R = 0$.  Both of those special cases lead to degenerate eigenspaces; in the former, the system is clearly purely Majorana from the beginning, and in the latter the system is purely Dirac.  Thus we can see that even though it is possible to include a Dirac neutrino mass term, neutrinos will be Majorana particles unless $m_L = m_R = 0$.  Similar results hold in the three-flavor case~\cite{RMPbb0n}.

We have spoken of the possibility that the Majorana mass term comes from a tree-level Lagrangian term.  However, this would be a surprising result.  The Majorana fields would not have the same quantum number under the $SU(2)_L\times SU(2)_R$ symmetry, meaning that either neutrinos would need to choose between the Majorana and Dirac terms or some larger symmetry would need to take the place of $SU(2)_L\times SU(2)_R$.  It is viewed as more likely that neutrino mass is generated through an effective interaction.  A natural mechanism to generate neutrino mass called the see-saw mechanism was developed around 1980~\cite{PhysRevLett.44.912,GellMann:1980vs}.  In this scheme we presume that $m_R \gg m_D \gg m_L$.  The Dirac term could come from the Higgs mechanism, so we expect $m_D$ to occupy the 1 MeV energy range typical of other leptons and quarks.  The large mass $m_R$ of the right-handed neutrino $\nu_R$ is meant to explain the absence of $\nu_R$ and $\overline{\nu_R^c}$ from all observations.  When we take $m_L = 0$ and diagonalize the single-flavor neutrino mass matrix $\mathcal{M}$ we obtain:
\begin{align}
\mathcal{M} &= \begin{pmatrix}0 & m_D \\ m_D & m_R \end{pmatrix}\\
&\propto \begin{pmatrix} \mathrm{i} & m_D/m_R \\ -\mathrm{i}m_D/m_R & 1 \end{pmatrix}
\begin{pmatrix} -m_D^2/m_R & 0 \\ 0 & m_R\end{pmatrix}
\begin{pmatrix}-\mathrm{i} & \mathrm{i}m_D/m_R \\ m_D/m_R & 1 \end{pmatrix}.
\end{align}
In other words, if we presume that there is a Dirac neutrino mass similar to the masses of other leptons and a right-handed Majorana neutrino mass $m_R \gg m_D$, then in another basis a small left-handed Majorana neutrino mass $m_D^2/m_R$ is generated.  This is widely considered to be the most natural mechanism for explaining the lightness of the neutrinos observed in nature~\cite{RMPbb0n,mohapatra1998massive}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{halflife_vs_year.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{A selection of $\beta\beta 0\nu$ half-life limits versus the publication year of the limit.  Colors indicate which isotope is under study.  Open circles indicate experiments which have not yet concluded data-taking.  Data is from ~\cite{
PhysRevC.85.045504,PhysRevLett.110.062502,bb0nSearch2012,PhysRevLett.111.122503,
KlapdorDissent,PhysRevLett.83.41,PhysRevC.59.2108,PhysRevD.65.092007,Andreotti2011822,PhysRevC.78.035502,
PhysRevLett.95.142501,Arnaboldi2004260,Baudis1997219,PhysRevLett.95.182302,NEMO2004,NEMO3-2013-100Mo,
doi:10.1142/S0217732390001475,Bernabei200223,Balysh1994176,0954-3899-17-S-014,Arnaboldi2003167,
1112.0859,Baksan2006Gavriljuk,Luescher1998407,Alessandrello1992176,Alessandrello1994519,Alessandrello1998156,
Alessandrello200013,PhysRevD.48.1009,PhysRevLett.59.419,Bellotti1991193,Bellotti1989209,BellottiMilano1986,
Bellotti1984450,Bellotti198372,BellottiMilano1982,PhysRevD.45.2548,PhysRevC.63.065501,0305-4616-13-6-012,
Ejiri199685,Ejiri199117,PhysRevC.55.474,PhysRevLett.71.831,PhysRevLett.63.1671,Fisher1989257,
PhysRevLett.50.721,PhysRevC.34.666,PhysRevLett.53.141,PhysRevD.51.2090,Forster1984301,PhysRevC.56.2451,
Barabash1989273,Fiorini1967602,Fiorini1973,Barabanov:1986iz,Vasilev:1990gi,PhysRevC.38.895}.}
\label{fig:Halflife_vs_year}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

A selection of $\beta\beta 0\nu$ half-life limits are shown in figure~\ref{fig:Halflife_vs_year} for the $^{76}$Ge, $^{100}$Mo, $^{130}$Te, and $^{136}$Xe isotopes with the publication year of the result.  Although some results did exist before 1980, after the publication of the see-saw mechanism in~\cite{PhysRevLett.44.912,GellMann:1980vs} experimental interest in neutrinoless double-beta decay flourished, and we can see that for the last thirty years there has been steady progress in improving experimental sensitivity for many isotopes.  The discovery by \cite{SuperK} in 1998 of oscillation of atmospheric neutrinos verified that neutrinos do have finite mass of some sort, adding to the motivation to search for the $\beta\beta 0\nu$ process.  The following sections describe computational and experimental considerations associated with searches for $\beta\beta 0\nu$ decay.

\section{Double-beta Decay Nuclear Matrix Calculations}\label{sec:NuclearMatrixCalculations}

If the dominant mechanism of $\beta\beta 0\nu$ decay is a tree level neutrino mass term as shown in figure~\ref{fig:FeynmanBetaBeta0Nu}, then the rate of $\beta\beta 0\nu$ decay will also reflect the magnitude of the neutrino mass parameters.  To specify this relation precisely we need an understanding of the nuclear physics of the decaying isotope.  This section will identify the relevant quantities which must be computed and provide a survey of the computational approaches to estimating them.

For a tree-level Majorana interaction we can write the partial half-life $T^{0\nu}_{1/2}$ of an isotope which undergoes $\beta\beta 0\nu$ decay as
\begin{equation}\label{eqn:HalfLifeMatrixElementEqn}
\left[T^{0\nu}_{1/2}\right]^{-1} = G_{0\nu}(Q_{\beta\beta}, Z) \left| M_{0\nu}\right|^2 \left< m_{\beta\beta} \right>^2,
\end{equation}
where $G_{0\nu}(Q_{\beta\beta}, Z)$ is a phase-space factor coming from the range of possible output states, $M_{0\nu}$ is a nuclear matrix element, and $\left< m_{\beta\beta} \right>$ is the effective $\beta\beta 0\nu$ neutrino mass which will be defined in section~\ref{sec:NucPhysConstraintsFromBB0N}.

The two factors $G_{0\nu}(Q_{\beta\beta}, Z)$ and $M_{0\nu}$ may be computed by a variety of methods, and it is tempting to draw nuclear matrix elements and phase factors from different publications.  However, this must be done carefully: scaling factors can be absorbed from one into the other, and if nuclear matrix elements and phase space factors are calculated using different conventions, the result of combining them will not be correct.

For example, in early calculations, the nuclear matrix element and phase-space factor were generally computed in units of $\mathrm{fm}^{-1}$ and $\mathrm{yrs}^{-1} \mathrm{fm}^2$, respectively.  Starting in the mid-1980s common practice shifted to multiply the nuclear matrix element by the nuclear radius $R_0 \propto A^3$, where $A$ is the number of nucleons, and divide the phase-space factor by $R_0^2$, making the nuclear matrix element unitless and the phase-space factor have units of $\mathrm{yrs}^{-1}$. With this convention care must be taken that the nuclear radius used in both calculations is the same, whereas for many years different authors might choose $R_0 = 1.1 A^3$ fm or $R_0 = 1.2 A^3$ fm without specifying that choice~\cite{PhysRevC.73.028501}.  Modern practice is now that the value of $R_0$ is explicitly specified in any matrix element or phase-space factor calculations. Another convention is whether the fourth power of the axial vector current $g_A^4$ should be included with the phase space factor, nuclear matrix element, or separately as its own factor of equation~\ref{eqn:HalfLifeMatrixElementEqn}, and again one must be careful to understand the chosen conventions before combining results from different sources~\cite{PhysRevC.87.014315}.

The phase-space factor $G_{0\nu}(Q_{\beta\beta}, Z)$ accounts for the phase space of the final state of $\beta\beta 0\nu$ decay.  This includes two outgoing electrons and the final state nucleus.  The mass of the outgoing nucleus is always much larger than the masses of the neutrinos, so nearly all momentum will be carried by the two electrons.  Each contributes a phase space integral of the form $\int_0^{p_{max}} \mathrm{d}\cdot p pE$, which individually contribute a factor proportional to $p_{max}^3$.  Considering the system in the rest frame of the initial state nucleus, the sum of the two electron momenta are constrained to be equal to zero, so the combined phase space integral for both electrons is proportional to
\begin{equation}
\int_0^{p_{max}} \mathrm{d}p_1 p_1E_1 \int_0^{p_{max}} \mathrm{d}p_2 p_2E_2 \delta(p_2-p_1) \propto p_{max}^5.
\end{equation}
As a result, the phase space factor $G_{0\nu}(Q_\beta\beta, Z)$ of $\beta\beta 0\nu$ decay will be proportional to $Q^5$, where $Q$ is the total energy of the decay~\cite{mohapatra1998massive}.  The strong dependence of $G_{0\nu}(Q_\beta\beta, Z)$ on $Q$ means that isotopes which have high $Q$-values will be expected to have shorter $\beta\beta 0\nu$ half-lives than isotopes with low $Q$-values, which is one reason that most $\beta\beta 0\nu$ searches focus on high-$Q$ isotopes.

Another important contribution to the phase space factor come from the electric potential of the atomic nucleus and its atomic electrons.  The potential $V(r)$ experienced by the escaping electron as a function radius is approximated by:~\cite{PhysRevC.87.014315}
\begin{equation}
V(r) = -Z \alpha \hbar c \cdot \begin{cases}
1/r & r \ge R_0 \\
\left(3-(r/R_0)^2\right)/2R_0 & r < R_0,
\end{cases}
\end{equation}
where $Z$ is the nuclear charge, $\alpha$ is the fine structure constant, $\hbar$ is Planck's constant, $c$ is the speed of light, and $R_0$ is the radius of the nucleus.  Higher-order corrections include the change in nuclear charge due to the decay and the density of the electron cloud (which includes angular asymmetries).  A modern treatment can be found in~\cite{PhysRevC.87.014315}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{SenseAndSensitivityNME.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Nuclear matrix element calculations for a variety of isotopes.  The behavior is relatively stable from isotope to isotope due to the short-range interaction for $\beta\beta 0\nu$ decay.  Figure from~\cite{1475-7516-2011-06-007}.}
\label{fig:MatrixElementComparisonVsAtom}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The nuclear matrix element $M_{0\nu}$ describes the transition rate from the initial to the final nuclear state of the decay process.  The transition may be treated as a two-step process:
\begin{align}
(Z,A) &\rightarrow (Z+1,A) + e^{-} + \bar{\nu}_e \notag \\
\nu_e + (Z+1,A) &\rightarrow (Z+2,A) + e^{-},
\end{align}
where the intermediate state is a virtual state because it does not conserve energy.  Because the two converted protons must be near each other within the nucleus for the neutrino interaction to occur, the $M_{0\nu}$ is not very sensitive to the variations in nuclear structure between elements; this is in contrast to the transition probability for $\beta\beta 2\nu$, $M_{2\nu}$, for which the neutrons may be well-separated within the nucleus~\cite{PetrVogel0nuAnd2nuMatrixElements}.  This similarity can be observed in figure~\ref{fig:MatrixElementComparisonVsAtom}.

%The three components to a nuclear matrix calculation are:
%\begin{itemize}
%\item Computation of the ground-state nuclear structure of the initial nucleus.
%\item Computation of the ground-state nuclear structure of the final nucleus.
%\item Evaluation of the $\beta\beta 0\nu$ transition operator.
%\end{itemize}
%All three of these components can only be performed approximately, and the choice of approximations accounts for differences up to a factor of 2-3 between matrix element calculations~\cite{RMPbb0n}.  Some of the available choices are outlined here.

To compute the nuclear matrix elements $M_{0\nu}$, two main approaches exist: the quasi-random phase approximation (QRPA) and the nuclear shell model.  In the simpler random phase approximation (RPA), the transition from initial to virtual intermediate state and from intermediate to final state are produced by operators of the form $p^+ n$, where $p^+$ represents the proton creation operator and $n$ represents the neutron annihilation operator.  The goal is to make these operators more bosonic (and hence more amenable to treatment in a statistical fashion), and so these operators are diagonalized in a new basis which acts on pairs of nuclei and obeys bosonic commutation rules.  QRPA is similar to RPA, but additionally takes into account the preference of like nuclei to pair together.  This is accomplished at the cost of breaking nucleon number conservation in the operator, but nucleon number is still preserved on average, and the modification can have a significant impact on the result~\cite{RMPbb0n}.

The other common approach to nuclear matrix calculations, the nuclear shell model, attempts to capture more fully the dynamics of a nuclear system by including the full nucleon state space and using nucleon-pair (or higher-order) interactions which can be measured empirically from small nuclei.  The disadvantage compared to QRPA is its enormous computational demands which make a full shell model treatment of relevant $\beta\beta 0\nu$ nuclei impossible at this point.  However, it is possible to perform nuclear shell model calculations with a severely truncated state space, and for nuclei whose shape is close to spherical the results can be reasonable.  As computational power increases it is likely that the shell model will overtake QRPA methods, but for now only a few large-scale shell model calculations of $\beta\beta 0\nu$ matrix elements have been undertaken~\cite{RMPbb0n}.

One important component of both the shell model and QRPA techniques is feedback from experimental data.  This can serve two purposes.  The first is validation: because both approaches make significant approximations, the results of those approximations must generally be tested empirically to ensure they do not have adverse consequences for the accuracy of the result.  Observables such as nuclear energy levels and emission spectra are commonplace; only recently, however, have precision decay rates for $\beta\beta 2\nu$ decay begun to appear in the literature~\cite{bb2nEXO2014}.  Although there are many differences between the calculations of $M_{2\nu}$ and $M_{0\nu}$, still it is currently the best source of validation for $M_{0\nu}$ calculations available, so this can help researchers to improve their understanding of the appropriate approximations for double-beta decay matrix elements.

The other benefit of experimental data is that it can help to constrain input parameters to the calculations.  In QRPA, a parameter called $g_{pp}$ controls the strength of phonon-phonon interactions and needs to be fixed from experimental data; the double-beta matrix elements depend strongly on the value of this parameter, and precision observations of $\beta\beta 2\nu$ can be used to constrain its value~\cite{RMPbb0n,PetrVogel0nuAnd2nuMatrixElements}.  In the shell model, the extreme truncation of the single-particle state space results in a need to renormalize the axial vector current $g_A$, which requires a related observable to control that renormalization; precision observations of $\beta\beta 2\nu$ can be an appropriate method to constrain that value as well.  One disadvantage to the use of $\beta\beta 2\nu$ decay rates to constrain parameters which are input to calculations is that the same observations can no longer be used to validate the approximations for double-beta decays~\cite{PetrVogel0nuAnd2nuMatrixElements}.  A description of efforts to obtain new experimental data useful for nuclear theory can be found in \cite{ZuberWorkshop}.

When the results from modern shell model and QRPA calculations are compared, it is found that the results can differ by as much as a factor of 2-3~\cite{RMPbb0n}.  Provided that there is no systematic effect which impacts both methods similarly, this permits us to understand in a rough way the level of accuracy provided by these calculations.  Progress in reducing these differences continues primarily by increasing the number of single-particle states that can be included, but it is clear that for the near term the search for $\beta\beta 0\nu$ will only be able to set rough limits on Majorana neutrino mass.

\section{Neutrino Flavor Physics}\label{sec:NeutrinoFlavorPhysics}

Neutrinos are known to exist in three flavors, or eigenstates which are diagonal with respect to the lepton interaction terms of the Standard Model.  These flavors are $\nu_e$, $\nu_\mu$, and $\nu_\tau$; they interact, respectively, with $e$, $\mu$, and $\tau$ leptons.  We also expect there to be a basis in which the neutrino mass matrix is diagonalized, and these two bases are not the same.  We call the mass eigenstates $\nu_1$, $\nu_2$, and $\nu_3$ with respective masses $m_1$, $m_2$, and $m_3$.  The unitary operator which translates between the two bases is specified by:
\begin{equation} \label{eqn:ShortDefinitionOfU}
\begin{pmatrix} \nu_e \\ \nu_\mu \\ \nu_\tau \end{pmatrix}
=
\mathbf{U}
\begin{pmatrix} \nu_1 \\ \nu_2 \\ \nu_3 \end{pmatrix}
=
\begin{pmatrix}
U_{e1} & U_{e2} & U_{e3} \\
U_{\mu1} & U_{\mu2} & U_{\mu3} \\
U_{\tau1} & U_{\tau2} & U_{\tau3}
\end{pmatrix}
\begin{pmatrix} \nu_1 \\ \nu_2 \\ \nu_3 \end{pmatrix}.
\end{equation}

These formalities are uninteresting if the neutrino sector is massless, since the mass eigenstates are degenerate.  However, in the case where neutrinos are massive, we can see that neutrinos which are created in one flavor eigenstate will oscillate between the flavor eigenstates with a period which depends on the differences between masses, as is typical in an N-state quantum system.  In the neutrino sector, we can more specifically state that the probability for a transition from flavor $\alpha$ to flavor $\beta$ will be:~\cite{RevModPhys.75.345}
\begin{equation}
P_{\alpha \beta} = \delta_{\alpha \beta} - 4 \sum_{i=1}^2 \sum_{j=i+1}^3 Re \left[ U_{\alpha i} U^{*}_{\beta i} U^{*}_{\alpha j} U_{\beta j} \right] sin^2 \left( \frac{ \left[m_i^2 - m_j^2\right]L}{4E} \right)
\end{equation}
where $L$ is the distance (or time, in $c=1$ units) between neutrino source and destination, and $E$ is the relativistic energy of the emitted neutrinos.

The transition probability is sensitive to the masses of the neutrinos, but only in the form $\left| m_i^2 - m_j^2\right|$.  These measurements have now been performed in a variety of neutrino oscillation experiments, and the best current constraints are $\left| m_1^2 - m_2^2 \right| = (7.50 \pm 0.20) \cdot 10^{-5} \text{eV}^2$~\cite{PhysRevD.83.052002} and $\left| m_2^2 - m_3^2 \right| = 2.32^{+0.12}_{-0.08} \cdot 10^{-3} \text{eV}^2$~\cite{PhysRevLett.106.181801}.

However, oscillation experiments cannot constrain the overall mass scale of neutrinos; they can only set a conservative lower limit that $max(m_2, m_3) \ge 0.048$ eV if we assume one of $m_2$ or $m_3$ is zero.  Furthermore, they do not establish the sign of the difference.  We can see that $m_1$ and $m_2$ are fairly close together, and $m_3$ is significantly different; but we cannot see whether $m_3$ is larger or smaller than the other two masses.  We refer to the situation with $m_1 \simeq m_2 \ll m_3$ as the normal hierarchy, and $m_3 \ll m_1 \simeq m_2$ as the inverted hierarchy; the regime where $m_1 \simeq m_2 \simeq m_3 \gg \left| m_2^2 - m_3^2 \right|$ is called the degenerate region.  Distinguishing between these three situations is one of the significant open questions in neutrino physics because of its impact on observable quantities.

\section{Particle Physics Constraints}\label{sec:ParticlePhysicsConstraints}

To produce more detailed constraints on neutrino physics, it is generally useful to provide a parametrization of the mixing matrix $\mathbf{U}$ from equation~\ref{eqn:ShortDefinitionOfU}.  The standard parametrization is:
\begin{align} \label{eqn:LongDefinitionOfU}
  \mathbf{U} &=
    \begin{pmatrix}
    U_{e1} & U_{e2} & U_{e3} \\
    U_{\mu1} & U_{\mu2} & U_{\mu3} \\
    U_{\tau1} & U_{\tau2} & U_{\tau3}
    \end{pmatrix} \notag \\
  &= \begin{aligned}[t]
      \begin{pmatrix}
      1 & 0 & 0 \\
      0 & cos(\theta_{23}) & sin(\theta_{23}) \\
      0 & -sin(\theta_{23}) & cos(\theta_{23})
      \end{pmatrix}
      \begin{pmatrix}
      cos(\theta_{13}) & 0 & sin(\theta_{13})e^{-i\delta} \\
      0 & 1 & 0 \\
      -sin(\theta_{13})e^{i\delta} & 0 & cos(\theta_{13})
      \end{pmatrix} \times \\
      \begin{pmatrix}
      cos(\theta_{12}) & sin(\theta_{12}) & 0 \\
      -sin(\theta_{12}) & cos(\theta_{12}) & 0 \\
      0 & 0 & 1
      \end{pmatrix}
      \begin{pmatrix}
      e^{i\alpha_1/2} & 0 & 0 \\
      0 & e^{i\alpha_2/2} & 0 \\
      0 & 0 & 1
      \end{pmatrix}.
    \end{aligned}
\end{align}
Out of the six parameters used in defining this matrix, the only ones which have been measured are the three mixing angles $sin^2(2\theta_{12}) = 0.857^{+0.023}_{-0.025}$~\cite{PhysRevD.83.052002}, $sin^2(2\theta_{13}) = 0.089 \pm 0.010 \text{(stat)} \pm 0.005\text{(sys)}$~\cite{1674-1137-37-1-011001}, and $sin^2(2\theta_{23}) > 0.95$~\cite{PhysRevLett.107.241801}.  The Dirac phase $\delta$ is in principle observable from oscillation experiments, but no current experiments have achieved the sensitivity necessary to accomplish this.  The Majorana phases $\alpha_1$ and $\alpha_2$ cannot be extracted from neutrino oscillations~\cite{RMPbb0n}.

The sum of the three mass eigenstates, $M = \sum m_i$, can be constrained by cosmological observations.  This constraint, like all cosmologically-based constraints, is model-dependent; it relies on the expectation that low-mass, hot forms of dark matter similar to neutrinos promote the formation of large-scale structures in the early universe by allowing extremely remote regions of matter to remain in thermal equilibrium.  Recent results from Planck combined with WMAP and baryon acoustic oscillations have restricted $M < 0.230$ eV with $95\%$ confidence~\cite{CosmologicalLimits}.  Considering the assertion in section~\ref{sec:NeutrinoFlavorPhysics} that the heaviest neutrino must have a mass no less than $0.048$ eV, we can see that this cosmological constraint pushes $M$ to within a factor of five of its lower limit. Taken at face value, the Planck measurement is the strongest existing constraint on the absolute mass scale of neutrinos.

Closer to home, the mass of neutrinos is also reflected in $\beta$ decay, $d \rightarrow u + e^- + \bar{\nu}_e$.  The total energy of the daughter products is known, and is shared between the electron and antineutrino; the minimum energy of the antineutrino is its rest mass, so by searching for the maximum energy of the electron we can simultaneously measure the rest mass of the neutrino.  The electron anti-neutrino emitted from beta decay is a mixture of all three mass eigenstates; since no current or planned experiment has sufficiently good energy resolution to resolve the separate endpoints from the three neutrino masses, we can instead write an effective rest mass of an electron antineutrino as:~\cite{RMPbb0n}
\begin{equation} \label{eqn:DefinitionOfMBeta}
\left< m_\beta \right>^2 = \sum_i m_i^2 \left| U_{ei} \right|^2.
\end{equation}

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{TritiumSpectrum.jpg}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The electron spectrum of Tritium ($^3H$) $\beta$ decay.  The endpoint contains only a small fraction of the total statistics.  Figure from~\cite{Angrik:2005ep}.}
\label{fig:TritiumSpectrum}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

To measure $\left< m_\beta \right>$ we must observe the electron energy spectrum of beta decays at the endpoint, which is complicated by the fact that this portion of the electron spectrum contains only a small fraction of the total electron statistics.  Tritium ($^3H$) is commonly used for these experiments because it has a medium-length halflife of $12.3$ years and an extremely low $\beta$ decay endpoint energy of $18.6$ keV, which maximizes the relative shift in endpoint energy due to $\left< m_\beta \right>$.  Figure~\ref{fig:TritiumSpectrum} shows that for $\left< m_\beta \right> = 1$ eV in Tritium, experiments must observe a shift in the spectrum which affects only about one decay in $5 \cdot 10^{12}$, making this level of sensitivity extremely difficult to achieve.  The best existing limit from $\beta$ decay is $\left<m_\beta\right> < 2.05$ eV, with $95\%$ confidence, from the Troitsk experiment which ran from 1994 to 2004~\cite{OldTritium}.  The KATRIN experiment hopes to achieve a sensitivity of $0.2$ eV, and is expected to begin taking data in 2015~\cite{NewTritium,NewTritiumTimeline}.

\section{Nuclear Physics Constraints from \texorpdfstring{$\beta\beta 0\nu$}{Neutrinoless Double-Beta}}\label{sec:NucPhysConstraintsFromBB0N}

As stated in equation~\ref{eqn:HalfLifeMatrixElementEqn}, it is possible to relate the rate of $\beta\beta 0\nu$ decay to the effective Majorana neutrino mass $\left< m_{\beta\beta} \right>$ by:
\begin{equation}\label{eqn:HalfLifeMatrixElementEqn_thelatter}
\left[T^{0\nu}_{1/2}\right]^{-1} = G_{0\nu}(Q_{\beta\beta}, Z) \left| M_{0\nu}\right|^2 \left< m_{\beta\beta} \right>^2,
\end{equation}
where the phase-space factor $G_{0\nu}(Q_{\beta\beta}, Z)$ and nuclear matrix element $M_{0\nu}$ have been described in section~\ref{sec:NuclearMatrixCalculations}.  We can now proceed to relate the effective Majorana neutrino mass to the parametrization of section~\ref{sec:NeutrinoFlavorPhysics}.  We follow this relation with a discussion of the considerations which affect a $\beta\beta 0\nu$ decay search and a summary of the current state of the field.

The effective Majorana neutrino mass $\left< m_{\beta\beta} \right>$ comes from a combination of the three mass eigenvalues described in section~\ref{sec:NeutrinoFlavorPhysics}.  It takes the form:
\begin{equation} \label{eq:DestructiveMassInteraction}
\left< m_{\beta\beta} \right> = \left|\sum_k m_k U_{ek}^2\right|.
\end{equation}
Unlike $\left< m_\beta \right>$, which is an incoherent sum of strictly positive terms in equation~\ref{eqn:DefinitionOfMBeta}, we can see that $\left< m_{\beta\beta} \right>$ is a coherent sum of terms, each of which may have arbitrary complex phase which may increase or decrease the result of equation~\ref{eq:DestructiveMassInteraction}.  In other words it is possible, even if neutrinos do have Majorana mass, for $\left< m_{\beta\beta} \right>$ to be arbitrarily small if $\mathbf{U}$ is tuned to produce cancellations between terms, specifically by tuning the Majorana phases and selecting the normal rather than inverted mass hierarchy~\cite{PDG}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{PDGNeutrinoMassBounds.eps}
\end{center}
\caption{The relationship between the effective Majorana mass $\left<m_{\beta\beta}\right>$ and other fundamental neutrino quantities: the lightest neutrino mass eigenstate $m_{min}$, the sum of mass eigenstates $M = \sum m_i$, and the effective single-beta-decay neutrino mass $\left<m_{\beta}\right>$.  Black (magenta) lines indicate the allowed region for the inverted (normal) hierarchy; red (blue) hatches indicate uncertainty for the inverted (normal) hierarchy due to the unknown CP-violating and Majorana phases $\delta$, $\alpha_1$, and $\alpha_2$~\cite{PDG}.}
\label{fig:NeutrinoMassBounds}
\end{figure}

It is possible to relate the observables $\left< m_{\beta\beta} \right>$, $\left< m_\beta \right>$, and $M$ within the model for $\mathbf{U}$ of equation~\ref{eqn:LongDefinitionOfU}.  These relations are shown in figure~\ref{fig:NeutrinoMassBounds}; we recall from section~\ref{sec:ParticlePhysicsConstraints} that beta spectrum measurements constrain $\left<m_\beta\right> < 2.05$ eV and cosmological observations constrain $M < 0.230$ eV, both with $90\%$ confidence.  The strongest constraints on $\left<m_{\beta\beta}\right>$ from $\beta\beta 0\nu$ searches place $\left<m_{\beta\beta}\right> < 0.15-0.4$ eV, depending on the choice of matrix element calculations.  We can see that if the cosmological limits are to be trusted, they provide the strongest constraints on the neutrino mass parameters; however, all three observables are complementary, and the wide range of experimental approaches means that systematic effects are unlikely to be shared by all three methods.  The figure shows the relation between these parameters in blue for the normal hierarchy and red for the inverted hierarchy; we note that it is only for the normal hierarchy at equation~\ref{eq:DestructiveMassInteraction} can lead to an extremely small $\left< m_{\beta\beta} \right>$.  In the case of the inverted hierarchy we can see $\left< m_{\beta\beta} \right> > 0.013$ eV, only an order of magnitude lower than the current limits.

The sensitivity of an experiment for measuring $T_{1/2}^{0\nu}$ can be described by the approximate formula:~\cite{RMPbb0n}
\begin{equation}\label{eqn:ApproxHalflifeSensitivity}
T_{1/2}^{0\nu}(n_\sigma) = \frac{4.16 \cdot 10^{26} yrs}{n_\sigma} \left( \frac{\epsilon a}{W}\right) \sqrt{\frac{Mt}{b \Delta E}},
\end{equation}
where $M$ is the mass of material, $a$ is the isotopic enrichment, $W$ is the molecular mass of the material in atomic units, and $t$ is the live-time of the experiment; $\epsilon$ is the signal efficiency, $b$ is the background rate (in counts per kg keV year, or some similar units), and $\Delta E$ is the energy resolution of the detector at the $Q$-value; and $n_\sigma$ is the desired confidence limit, in sigmas, where the standard $90\%$ confidence limit will require $n_\sigma = 1.64$.  The scaling of this equation is most accurate when the energy resolution is much smaller than the $Q$-value and the background is uniformly distributed in energy; however, when combined with phase factor and nuclear matrix element estimates it roughly allows us to compare the sensitivity of different $\beta\beta 0\nu$ experiments.

According to equation~\ref{eqn:ApproxHalflifeSensitivity} we should prefer experiments for which:
\begin{itemize}
\item A large quantity of highly-enriched isotope can be obtained.
\item Signal detection is highly efficient.
\item Background contamination around the $Q$-value is small, and does not scale with detector mass.
\item Good energy resolution can be achieved.
\end{itemize}

The leading experiments which are planned or currently searching for $\beta\beta 0\nu$ are as follows:
\begin{enumerate}
\item The leading search for $^{76}$Ge comes from the GERDA experiment at the Gran Sasso Laboratory in Italy.  GERDA consists of an array of cryogenic germanium detectors with charge readout.  $^{76}$Ge has a $Q$-value of only 2039 keV, giving it a lower phase factor than other popular materials.  It also is expensive to grow large uniform crystals of germanium; this means that it is difficult for a germanium experiment to take advantage of self-shielding from external radioactive backgrounds.  However, its strongest advantage is its excellent energy resolution: GERDA has achieved an energy resolution at its $Q$-value of 1.1-1.7 keV ($\sigma$) with its newer crystals~\cite{PhysRevLett.111.122503}.
\item The best results with $^{130}$Te have come from the CUORICINO experiment which ran at the Gran Sasso Laboratory in Italy.  CUORICINO was a bolometric experiment: it cooled tellurium crystals to extremely low temperatures where the heat capacity becomes small, so that a decay inside the tellurium creates a measurable change in temperature.  Similarly to $^{76}$Ge, it is expensive to grow large crystals of tellurium, so most tellurium experiments use an array of detectors.  $^{130}$Te has a $Q$-value of 2528 keV, somewhat larger than that of $^{76}$Ge, but the energy resolution of the enriched crystals was 2.1-10.6 keV ($\sigma$), depending on the crystal.  CUORICINO stopped running in 2008~\cite{Andreotti2011822}; planned experiments in $^{130}$Te include CUORE~\cite{1402.0922} and SNO+~\cite{1402.1170}, both of which anticipate data-taking beginning in 2015.
\item The two leading experiments in $^{136}$Xe are KamLAND-Zen, located at the Kamioka Observatory, and EXO-200, located in the WIPP facility.  $^{136}$Xe also has a $Q$-value of 2458 keV, giving it a higher phase-space factor than $^{76}$Ge but not $^{130}$Te.  KamLAND-Zen dissolves its xenon in a liquid scintillator and observes only scintillation energy; it achieves an energy resolution of 103 keV ($\sigma$), modest compared to the resolution achieved in tellurium and germanium, but its monolithic nature allows it to keep backgrounds low~\cite{PhysRevLett.110.062502}.  EXO-200 is a pure liquid xenon detector which observes both ionization and scintillation. This, combined with analysis improvements described in the present work, allows it to achieve a somewhat better energy resolution of 38 keV ($\sigma$)~\cite{NewEXObb0nPaper_2014}.
\end{enumerate}

\begin{table}
\begin{center}
\begin{tabular}{|rccccr|}
\hline Isotope & $T_{1/2}^{0\nu}$ (years) & $M_{0\nu}$ & $G_{0\nu}$ (yrs${}^{-1}$) & $\left<m_{\beta\beta}\right>$ (eV) & Collaboration \\ \hline
$^{48}$Ca & $5.8 \cdot 10^{22}$ & 2.28 & $2.48 \cdot 10^{-14}$ & $3.7$ & ELEGANT IV \cite{ElegantIV}\\
$^{76}$Ge & $2.1 \cdot 10^{25}$ & 5.98 & $2.36 \cdot 10^{-15}$ & $0.24$ & GERDA \cite{PhysRevLett.111.122503} \\
$^{82}$Se & $3.6 \cdot 10^{23}$ & 4.84 & $1.02 \cdot 10^{-14}$ & $1.1$ & NEMO-3 \cite{NEMO2011RandomOtherIsotopes}\\
$^{96}$Zr & $9.2 \cdot 10^{21}$ & 2.89 & $2.06 \cdot 10^{-14}$ & $8.0$ & NEMO-3 \cite{Argyriades2010168}\\
$^{100}$Mo & $1.1 \cdot 10^{24}$ & 4.31 & $1.59 \cdot 10^{-14}$ & $0.56$ & NEMO-3 \cite{NEMO3-2013-100Mo}\\
$^{116}$Cd & $1.6 \cdot 10^{22}$ & 3.16 & $1.67 \cdot 10^{-14}$ & $6.1$ & NEMO-3 \cite{NEMO2011RandomOtherIsotopes}\\
$^{130}$Te & $3.0 \cdot 10^{24}$ & 4.47 & $1.42 \cdot 10^{-14}$ & $0.34$ & Cuoricino \cite{PhysRevC.78.035502}\\
$^{136}$Xe & $1.1 \cdot 10^{25}$ & 3.67 & $1.46 \cdot 10^{-14}$ & $0.22$ & EXO-200 \cite{NewEXObb0nPaper_2014}\\
$^{136}$Xe & $1.9 \cdot 10^{25}$ & 3.67 & $1.46 \cdot 10^{-14}$ & $0.16$ & KamLAND-Zen \cite{PhysRevLett.110.062502}\\
$^{150}$Nd & $1.8 \cdot 10^{22}$ & 2.74 & $6.30 \cdot 10^{-14}$ & $3.4$ & NEMO-3 \cite{PhysRevC.80.032501}\\
\hline
\end{tabular}
\end{center}
\caption{A listing of the strongest available $\beta\beta 0\nu$ limits; all half-life and mass limits are quoted at $90\%$ confidence.  Limits on $\left<m_{\beta\beta}\right>$ are obtained using phase space factors from~\cite{PhysRevC.85.034316} and matrix elements from~\cite{PhysRevLett.109.042501}, both chosen for the completeness of their tabulations.  These sources explicitly factor out $g_A$; we use $g_A = 1.269$.  For $^{136}$Xe, both Kamland-Zen's published results and the EXO-200 results described in this work are included in the table.}
\label{tab:0nubb_limits}
\end{table}

Table~\ref{tab:0nubb_limits} tabulates the most current $T_{1/2}^{0\nu}$ limits in all $\beta\beta 0\nu$ isotopes for which $\beta\beta 0\nu$ limits have been published.  Representative limits on $\left< m_{\beta\beta}\right>$ are also included; these come from one particular set of phase space factors and nuclear matrix element calculations performed using the interacting-boson model; this pair of sources for calculations is made because they include tabulations for a broad range of isotopes and permit comparisons across all available half-life limits~\cite{PhysRevC.85.034316,PhysRevLett.109.042501}.  However, one should bear in mind that errors in the matrix elements propagate to errors in $\left< m_{\beta\beta}\right>$ of as much as a factor of two for each isotope.  We can see that although there are a favored set of isotopes, active and successful programs exist in a wide range of isotopes, and no one isotope is ideal in all respects.

As the table indicates, $^{136}$Xe has provided some of the strongest constraints on $\left< m_{\beta\beta}\right>$ in spite of its relatively modest energy resolution.  The present work demonstrates significant improvements to the energy resolution observed by the EXO-200 detector in $^{136}$Xe which have been achieved through offline denoising of the scintillation signals.  This denoising technique is applied to data from the detector, and results in a stronger limit on $T_{1/2}^{0\nu}$ from EXO-200 than could be obtained from the same data without denoising.  Chapter~\ref{ch:DenoisingResults} presents recent results from EXO-200 which benefit from this denoising technique, as well as increased livetime and other improvements.
