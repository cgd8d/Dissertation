\renewcommand{\thechapter}{7}
\chapter{EXO-200 Analysis and Results from Denoising}
\label{ch:DenoisingResults}

Data analyzed for the present work extends from October 5, 2011 to September 1, 2013.  In this chapter, sections~\ref{sec:ResultSimulation}-\ref{sec:ResultFitting} describe the basic elements of the EXO-200 analysis.  Section~\ref{sec:ResultResults} will present the results from this set of data.  In section~\ref{sec:ResultComparison} we compare the results obtained using the denoising scheme of chapter~\ref{ch:DenoisingTheory} to the results which would have been obtained without that algorithm applied, and demonstrate that denoising has contributed to the strength of our physics reach.

\section{Simulation}\label{sec:ResultSimulation}

Here we will describe the simulation of radioactive sources in the EXO-200 detector.  We begin by describing the framework for simulating the deposition of energy from primary decay particles into the liquid xenon and surrounding materials.  From there, we continue to describe the simplified electric field model which permits us to model the drift of ionization towards the anode wires.

\subsection{Simulation of Particles using GEANT}

To simulate the deposition of energy from primary decay particles, version 4.9.3p02 of the GEANT software package is used~\cite{Agostinelli2003250,1610988}.  This package includes a database containing attenuation properties of many common materials as well as detailed decay modes for most radioactive isotopes.  Angular correlations between gammas are not modeled with this version of the software, but that is expected to be a minor detail and will be corrected by upgrading to a newer version of GEANT in the future.  Forbidden beta decays are generated with an incorrect beta spectrum in the default GEANT software; this is addressed by the EXO collaboration by using our own beta spectra where appropriate.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{OGL_wireframe.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The GEANT simulation includes large-scale features of the EXO-200 detector, including the outer and inner lead wall (black), outer and inner cryostat and TPC legs (red), and the TPC itself (brown).  Components are assembled from simple geometrical shapes, and distant objects are only described coarsely~\cite{MCDocumentRun2a}.}
\label{fig:OGL_wireframevis}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth,clip=true,trim=20mm 30mm 35mm 125mm]{TPC_Cu_RayTracer.jpeg}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Detector components which are close to the liquid xenon are simulated in GEANT with far greater accuracy than distant objects, to reduce computational time~\cite{MCDocumentRun2a}.}
\label{fig:RayTracer_TPConly}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

A model of the EXO-200 detector is simulated in GEANT with a collection of simple geometric volumes, which can be composed to form more complicated structures.  Simulation speed decreases as more volumes are generated, and we expect that it is unimportant to simulate details much smaller than the distance between the detail and the liquid xenon.  We attempt to find a balance between accurate modeling of detailed features and simplification of distant features.  In figure~\ref{fig:OGL_wireframevis} we see the full geometry described in GEANT, where distant objects are constructed from a small number of geometric pieces.  Figure~\ref{fig:RayTracer_TPConly} shows how the TPC is described in GEANT, and we can see the level of detail is much greater.

To simulate particles, GEANT uses a Monte Carlo method.  It models particles as taking a sequence of steps, where each step is randomly generated; the probability distribution of each choice of step is determined by known properties of the particle, the energy of the particle, and the properties of the attenuating material.  The precise set of physical processes which determine the probability distribution is user-selected, and in EXO-200 has been chosen to include all processes which are significant within our energy range of 10 keV to 10 MeV~\cite{MCDocumentRun2a}.

Only energy which deposits in the liquid xenon is observable.  When primary decays are simulated far from the detector, it may be that most of those simulated events deposit no energy in the liquid xenon and are not observable; the simulation must continue running until a sufficient number of simulated events deposit energy in the liquid xenon.  This means that sources which are farther from the liquid xenon require significantly more computational time to accumulate a usable number of statistics.  We find that sources outside of the HFE are subject to $4.5$ attenuation lengths before reaching the liquid xenon, and events reaching the liquid xenon from the inner cryostat can only be simulated at $0.01$ Hz/core by this approach~\cite{MCDocumentRun2a}.

To improve this rate, importance sampling is employed for distant sources.  This approach consists of the following techniques to magnify statistics from a fixed simulation time:
\begin{itemize}
\item Low-energy beta and alpha particles outside of the TPC may be ``killed'', or prematurely eliminated from the simulation based on an expectation that they will not deliver any energy to the liquid xenon.
\item The detector is surrounded by importance sampling ``boundaries'': when a particle passes into a boundary it may be cloned (with a user-selected probability), where the rate of cloning is tracked by a corresponding decrease in particle ``weight.''
\item To avoid biasing the spectrum, it is also necessary to kill particles which pass out of a boundary with the same probability, and increase their weight accordingly.
\end{itemize}
This approach has the effect of using GEANT to simulate the properties of particles reaching the outermost boundary; then draw samples from that distribution and simulate the properties of particles reaching the next boundary; and so forth, amplifying the impact of statistics at each stage.  Further details on this approach can be found in~\cite{Dressel:642987}; it has the effect of increasing simulation speeds from the inner cryostat from $0.01$ Hz/core to a few Hz/core, and makes simulations of backgrounds from outside the lead wall feasible~\cite{MCDocumentRun2a}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{AllVessel_U238_single_multi_site_spec.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Energy spectra from $^{238}$U in the TPC vessel; single-site and multi-site energy spectra are shown separately~\cite{MCDocumentRun2a}.}
\label{fig:UGeantSpectra}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{AllVessel_Th232_single_multi_site_spec.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Energy spectra from $^{232}$Th in the TPC vessel; single-site and multi-site energy spectra are shown separately~\cite{MCDocumentRun2a}.}
\label{fig:ThGeantSpectra}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The result of these GEANT simulations is a measurement of the efficiency with which events from various sources reach the liquid xenon, and also an understanding of the energy and position distributions of energy deposits from these sources.  Representative spectra of our primary backgrounds, $^{238}$U and $^{232}$Th, are shown in figures~\ref{fig:UGeantSpectra} and \ref{fig:ThGeantSpectra} respectively.

\subsection{Digitization of Waveforms}\label{sec:ResultsDigitization}

After energy deposits are simulated using GEANT, it is necessary to model the conversion of those energy deposits into collection of scintillation and ionization, and then the generation of digitized waveforms resembling the waveforms which are collected in real data.

To generate a scintillation signal, a purely empirical model is used to estimate the relative signal magnitudes on the north and south APD planes.  This model only takes into account $Z$-dependence of the light collection, and does not incorporate the different light yields expected on each individual APD channel.  This model is therefore rather crude, and can only be used as a rough check on the signal-finding efficiency for scintillation as a function of energy.  Attempts to track optical photons in the TPC have met with only partial success which is insufficient to justify their significant computational cost.  Thus, the EXO simulations are unable to model most aspects of scintillation signals.  Section~\ref{sec:ResultFitting} will describe the methods used to cope with this aspect of simulation.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{ChargeDrift2DModel.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The wire planes are modeled in only two dimensions; charge drifts along the field lines, which are arranged to terminate only on the u-wires~\cite{MCDocumentRun2a}.}
\label{fig:TwoDimensionalWireModel}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

Simulation of charge signals is better-understood.  The detector is modeled by a two-dimensional geometry where, rather than one-dimensional wires arranged in two-dimensional planes, we have wire ``points'' at fixed voltage which are grouped in a one-dimensional pattern.  The v-wires are treated as stacked directly on top of the u-wires; it is impossible to model the true orientation of the v-wires which is rotated relative to the u-wires, but this has generally proven to be a negligible detail for us.  Only one TPC half is modeled because of the approximate mirror symmetry of the detector across the cathode.  Only a few wires are simulated in the second dimension because the electrostatic effect of any one wire will not extend beyond a few wire spacings.  This model is illustrated in figure~\ref{fig:TwoDimensionalWireModel}~\cite{MCDocumentRun2a}.

Electrostatic effects are simulated using the ANSYS Maxwell field simulator.  To simulate the electric fields in the detector, wires are treated as circles with a radius matching the approximate radii of the physical wires, with a constant voltage on their surfaces.  The APD plane and cathode plane are treated as constant-voltage boundaries, and a periodic boundary condition is established on the two remaining boundaries of the model geometry.

These electric fields can be used to trace the paths followed by charge deposits in the detector.  Charge deposits are drifted in small steps based on the direction of the electric field.  The speed of drift is taken from external measurements rather than from the magnitude of the electric field; in most of the bulk of xenon, the electric field and drift velocity are treated as constant, but near the u-wire plane the drift velocity is increased slightly to account for higher electric fields experienced in that region of the detector.  Charge attenuation due to finite purity can be modeled at this step, but generally is treated as infinite here; charge diffusion effects are ignored.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{WeightPotContoursU_WithE.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Weight potential of a u-wire channel consisting of three ganged wires.  Electric field lines are superimposed~\cite{MCDocumentRun2a}.}
\label{fig:UWireWeightPotential}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{WeightPotContoursV_WithE.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Weight potential of a v-wire channel consisting of three ganged wires.  Electric field lines are superimposed~\cite{MCDocumentRun2a}.}
\label{fig:VWireWeightPotential}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

We have discussed in section~\ref{sec:DetectorReadout} that charge is induced on the u-wires and v-wires; this means that we must record the amount of charge induced at each step along the drift path of the charge deposit.  This is done based on the Shockley-Ramo Theorem, which states that the change in induced charge $\delta q_i$ on an electrode $i$ is equal to:
\begin{equation}
\delta q_i = Q \delta W_i,
\end{equation}
where $Q$ is the total drifting charge and $W_i$ is the weighting potential of electrode $i$, defined as the potential which would be induced in our geometry if the potential on electrode $i$ were set to $1$ and the potential on all other boundaries were set to $0$~\cite{ShockleyPaper,1686997}.  Figures~\ref{fig:UWireWeightPotential} and~\ref{fig:VWireWeightPotential} illustrate the weighting potentials of a u-wire and v-wire channel, respectively.

Finally, the functions of integrated charge versus time must be converted to shaped digitized waveforms, and noise must be added.  The shaping and gain amplification is performed to match the electronics described in section~\ref{sec:DetectorReadout}.  To ensure accurate time shaping, the functions are all sampled at a bandwidth of $10$ MHz, and then downsampled after shaping to the nominal $1$ MHz.  Digitization is performed by converting voltages into units of ADC counts, then truncating to an integer value.  To model saturation effects, this number is pulled into the integer range $[0, 4096)$.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{pulse_comp.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Comparison between simulated and observed waveforms on a u-wire (left) and v-wire (right) from $^{228}$Th sources.  The events are chosen to have similar energies so that the magnitudes match~\cite{MCDocumentRun2a}.}
\label{fig:MCPulseComparison}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

We have the ability to add electronic noise to the signals by extracting a set of representative noise waveforms from real data.  These are taken from a large number of solicited triggers which have been checked for the absence of a coincident event.  To increase the number of noise signals available, the noise waveforms sampled from the detector are spliced together, and simulated events may draw their noise from any subrange of the spliced-together noise waveform.  This method provides our most accurate noise model because it can include all noise frequency peaks and channel correlations without requiring a full understanding of those features.

However, current analyses do not emply this method; instead, we simulate noise which is white (has a flat Fourier spectrum) before shaping, and then shaped to have a spectrum which roughly resembles the one observed in the detector.  Figure~\ref{fig:MCPulseComparison} compares waveforms observed from a sample event in data and simulation, and demonstrates that at high energies excellent agreement is achieved~\cite{MCDocumentRun2a}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{SS_Ra226_Campaign7.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Comparison between simulated and observed energy spectra (a) and standoff distance (b) in single-site $^{226}$Ra events from a source located at position S5~\cite{NewEXObb0nPaper_2014}.}
\label{fig:RaSourceMCComparison}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

In this way, we are able to generate simulated data which in most respects resembles real data collected from the detector.  The EXO-200 simulation has shown remarkable agreement with the detector in the properties of energy and cluster location, as illustrated in figure~\ref{fig:RaSourceMCComparison}.  A significant effort has been made to achieve excellent agreement between simulation and data, and the result is that we can claim a strong understanding of the behavior of the EXO-200 detector.

\section{Cluster Reconstruction}\label{sec:ResultReconstruction}

The first stage of data processing involves locating candidate pulses in its waveforms, and fitting for the magnitude and time of any candidate pulses which are found.  The methods of accomplishing this will be described in sections~\ref{sec:ReconPulseFinding} and~\ref{sec:ReconPulseFitting}.  We will complete our description of cluster reconstruction with a brief explanation of the approach to the clustering of waveform pulses into three-dimensional clusters in section~\ref{sec:ReconClustering}.

\subsection{Pulse Finding}\label{sec:ReconPulseFinding}

Pulse-finding proceeds in two steps.  First, on all waveforms a matched filter is applied to do a preliminary search on all channels.  Then, a secondary search is performed on the u-wire signals to improve sensitivity to multiple signals near in time.

The matched filter algorithm was first described by D.O. North in 1943~\cite{MatchedFilterPaper}.  It attempts to decide between the null hypothesis that a waveform $X[\tau]$ consists of only noise and an alternative hypotheses that the waveform contains both signal and noise:
\begin{equation}\begin{cases}
H_0: & X[\tau] = n[\tau]\\
H_1: & X[\tau] = s[\tau] + n[\tau].
\end{cases}\end{equation}
We search for a linear functional $L_0$ which will act on $X[\tau]$ and maximize the expected signal-to-noise ratio,
\begin{equation}
SNR = \frac{\left(L_0\left\{s[\tau]\right\}\right)^2} {\left<\left(L_0\left\{n[\tau]\right\}\right)^2\right>}.
\end{equation}
In other words, if a waveform is composed of only noise, the functional should result in a small value; however, if the waveform contains signal then the functional should result in a large value.

The functional which maximizes the expected signal-to-noise ratio can be expressed as:
\begin{equation}
L_0\left\{X[\tau]\right\} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\left\{X\right\}[k] \mathcal{F}\left\{s\right\}^{*}[k]}{\left<\mathcal{F}\left\{N\right\}[k]\mathcal{F}\left\{N\right\}^{*}[k]\right>}\right\}[0],
\end{equation}
where $\mathcal{F}$ represents the Fourier transform.  This expression has an added benefit that if instead we'd like to test the hypothesis that the translated signal $s[\tau - \Delta]$ is contained in the waveform, we can do so with the test statistic:
\begin{equation}
L_\Delta\left\{X[\tau]\right\} = \mathcal{F}^{-1}\left\{ \frac{\mathcal{F}\left\{X\right\}[k] \mathcal{F}\left\{s\right\}^{*}[k]}{\left<\mathcal{F}\left\{N\right\}[k]\mathcal{F}\left\{N\right\}^{*}[k]\right>}\right\}[\Delta].
\end{equation}
Finally, we can define a test statistic $T$ for the waveform $X[\tau]$ to have a signal anywhere with:
\begin{equation}
T = \max_{\Delta} L_\Delta\left\{X[\tau]\right\}.
\end{equation}
This expression can be computed efficiently using the fast Fourier transform.  The time of the pulse is guessed as the value of $\Delta$ which led to the largest value of $L_\Delta\left\{X[\tau]\right\}$.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MFExamp_Raw.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MFExamp_Applied.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{A u-wire waveform (left) and the output from operation of the matched filter (right).  The red line on the right indicates our signal threshold; the matched filter output exceeds the threshold, so this waveform is determined to contain a signal~\cite{ReconstructionDocument}.}
\label{fig:MatchedFilterApplication}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The first phase of pulse-finding performs a search using the matched filter on:
\begin{itemize}
\item All u-wire channels.
\item All v-wire channels.
\item The sum of all APD channels in the North plane.
\item The sum of all APD channels in the South plane.
\end{itemize}
An example application of the matched filter to a u-wire waveform is shown in figure~\ref{fig:MatchedFilterApplication}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MSF_Raw.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MSF_MatchedFilter.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MSF_Unshaped.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{MSF_Reshaped.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{A u-wire waveform composed of two signals near in time is shown (top left); the matched filter (top right) correctly detects the presence of signal, but does not detect the presence of two distinct signals.  At bottom left, the waveform is unshaped; at bottom right the waveform is reshaped with shorter differentiation times, leading to easier detection~\cite{ReconstructionDocument}.}
\label{fig:MultipleSignalFinderApplication}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The matched filter has proven to be an excellent metric for deciding whether a waveform has a signal on it or not.  However, it is not as effective at distinguishing between the cases where one or multiple signals occur on a waveform.  We would particularly like to be able to classify u-wire waveforms by the number of distinct signals they contain because these generally provide the best sensitivity for single-site/multi-site discrimination.  The matched filter output function is designed to have a tall peak in the presence of a signal, but it may also be a broad peak which is difficult to resolve as the sum of two distinct signal contributions.

To improve sensitivity to multiple signals in u-wire waveforms, a second pass is performed on u-wire channels using a multiple-signal finder.  This scheme consists of first, unshaping the waveform offline to remove the effect of the shapers; and second, reshaping the waveform using shorter differentiation times than employed by hardware.  In a multiple-signal waveform, this has the effect of damping signals faster to reduce their overlap in time.  It is then possible to search for signals using a simple threshold which is not as sensitive to small signals as the matched filter, but is capable of detecting additional signals to complement the matched filter.  This process is illustrated in figure~\ref{fig:MultipleSignalFinderApplication}, and is the last procedure for finding signals~\cite{ReconstructionDocument}.

\subsection{Pulse Fitting}\label{sec:ReconPulseFitting}

After finding signals, it is necessary to perform a fit to the expected signal shape.  Fits are performed allowing both the signal magnitude and signal shape to float freely, where only the initial guesses for these parameters come from the finding step.  The metric for fits is a simple chi-square between waveform data and the expected signal shape, where error on each point is estimated by the root-mean-square noise of the channel.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{U_Wire_Fit.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{V_Wire_Fit.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{APD_Sum_Fit.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Fits to data for a u-wire (top left), v-wire (top right), and summed-APD waveform (bottom).  The red model line indicates the time extent of the fit window~\cite{ReconstructionDocument}.}
\label{fig:ReconExampleFits}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

To minimize the impact of signal templates which may be imperfect, and also to reduce the effect which waveform noise may have on our fit, we do not fit the entire waveform to our signal model.  Instead, we use only an $80 \mu$s fit window in the case of v-wires and APDs, and a $180 \mu$s fit window in the case of u-wires.  The fit windows are illustrated in example events in figure~\ref{fig:ReconExampleFits}.

As signal times and magnitudes are extracted, it may be discovered at this phase of processing that two signals which were reported by the finding phase are extremely close together in time.  In this case, we can generally conclude that the finding phase inadvertently reported the same signal multiple times; the multiple candidate signals should then be merged together, and the waveform refit.  The specific criteria for merging candidate signals includes a combination of signal timing and fit errors from a preliminary fit where all candidate signals are included; details can be found in~\cite{ReconstructionDocument}.

\subsection{Clustering Pulses into Deposit Sites}\label{sec:ReconClustering}

The result of pulse finding and fitting is a list of pulse magnitudes and times for each channel.  It remains for reconstruction to combine this information into three-dimensional clusters, where each cluster should have a three-dimensional position, time, and estimates for charge and light signal magnitudes.

We begin by bundling groups of similar u-wire, v-wire, and APD pulses.  Multiple u-wire pulses may be bundled together when they occur on neighboring channels close together in time.  This may represent a cluster which occurred halfway between two u-wire channels and was split, with some charge being collected by each u-wire channel; it may also represent a combination of charge being collected on one channel and an induced charge signal on a nearby u-wire channel due to the proximity of the charge drift path to that neighboring channel.  V-wire pulses are bundled together based on a combination of signal magnitude and time to account for the observation that the passage of a single charge cloud will induce pulses on many nearby v-wires.  APD pulses are bundled based on time only, particularly to ensure that when a pulse is observed on both the North and South APD planes, the pulses are interpretted as originating from the same event in the detector.

We then attempt to join u-wire pulse bundles and APD pulse bundles into two-dimensional charge clusters, identified by their U-Z positions and the time of the energy deposit.  The algorithm to do this is simple: for each u-wire pulse bundle, we join it with the most recent APD pulse bundle in the event.  The maximum difference in time is the maximum drift time of charge clusters in the detector, $116 \mu$s.  To allow for fit errors in the time parameters of the APD and u-wire pulses, an additional $3 \mu$s allowance is added on either end of the permitted time difference; thus, an APD and u-wire pulse bundle may be joined if the APD occurred between $119 \mu$s before and $3 \mu$s after the u-wire collection time, and from these candidates the latest APD pulse bundle is selected for each u-wire pulse bundle.  The cluster's charge energy is the sum of the energies attributed to all of the u-wire pulses in the u-wire pulse bundle; no attempt is made to measure the cluster's scintillation energy.

Finally, we cluster v-wire pulse bundles with two-dimensional charge clusters to form three-dimensional clusters possessing X-Y-Z positions, deposit time, and charge energy.  The only information taken from the v-wire pulse bundle is the third position coordinate; it is not used to adjust our estimates of energy.  The challenge to this portion of clustering is that charge clusters may arrive at the same u-wire or v-wire at the same time, yet from two different locations.  For example, it may be that what appeared to be one u-wire pulse bundle in fact comes from two charge clusters with common U-Z positions and different V-positions.  So, in addition to searching for the most likely associations between u-wire/APD clusters and v-wire pulse bundles, we must consider the possibility of splitting a two-dimensional cluster or v-wire pulse bundle into pieces before performing this association.

To do this, every possible combination of u-wire/APD cluster and v-wire pulse bundle is considered, along with every possible choice of split for clusters or pulse bundles.  A likelihood is assigned to each possible combination; this likelihood takes into account:
\begin{itemize}
\item The ratio of v-wire and u-wire pulse magnitudes.  These pulse magnitudes are expected to be linearly related because they both are observations of the same charge drift cloud, and deviations from that linear relation are penalized by a decreased likelihood.
\item The time difference between the v-wire and u-wire pulses.  There is an expected drift time between the v-wires and u-wires which is measured from data; if the observed time difference deviates from this expectation, the likelihood is decreased to reflect a preference for a more realistic pairing.
\item The two-dimensional position which the pairing implies.  Due to the geometry of the detector, not all pairs of u-wire and v-wire can be hit by the same charge cluster.  When a u-wire and v-wire lie in different halves of the TPC, it is of course impossible for the same charge cloud to induce pulses on both.  In some cases a u-wire and v-wire may almost overlap, and it may be considered possible yet unlikely that the same charge cloud could induce pulses on both; these instances are penalized as well.
\end{itemize}
Further details of the clustering criteria may be found in~\cite{ReconstructionDocument}.

This section has described the situation where the pulses of an event may be reconstructed as a set of three-dimensional clusters with well-defined position, time, and charge energy.  Although every effort has been made to accomplish this whenever possible, in practice there are events for which some clusters may not be fully reconstructible.  Primarily this occurs because charge deposited on a u-wire near the edge of the detector where no v-wires exist; because a cluster fell below the threshold for v-wire signal-finding, or the event fell below the scintillation threshold; because the signal finder reported a false positive, yielding a non-physical pulse; or because an event has extremely high multiplicity and it is not possible to select a preferred clustering.  Such events are difficult to use in higher-level analyses and will generally be removed, with an associated energy-dependent efficiency loss of $9.1\%$~\cite{NewEXObb0nPaper_2014}.

\section{Energy Corrections}\label{sec:ResultEnergy}

Each cluster which is found can be assigned a preliminary energy estimate based on the fit magnitudes of its pulses.  However, to achieve the best possible energy resolution it is necessary to make an assortment of energy corrections.  These corrections will be identified in this section.  We will first describe the corrections which are made to charge energies on a cluster-by-cluster basis, and then the corrections made to the denoised scintillation energy.  Section~\ref{sec:RotatedEnergyCalibration} describes the calibration of a combined energy measurement using charge and light together.  Finally, in section~\ref{sec:RotatedEnergyResMeasurement} we describe the measurement of the energy resolution and compare the resolutions obtained with and without denoising.

\subsection{Charge Corrections}\label{sec:ResultEnergyCharge}

The preliminary charge cluster energy measurements come from the sum of the u-wire signal magnitudes described in section~\ref{sec:ReconPulseFitting}.  These magnitudes reflect the quantity of charge which was collected by the u-wire.

However, each u-wire channel has its own set of electronics, so pulse magnitudes must be adjusted by a gain correction which depends on the channel.  These gains are extracted from source data by selecting pair production events from the thorium source, in which a $2615$-keV gamma pair-produces an electron and positron, and the positron subsequently annihilates with another electron to emit two $511$-keV gammas.  The high multiplicity of these events makes it possible to select them accurately, and the process guarantees that a single $1593$-keV cluster at the site of the pair pruduction must be truly single-site because it originates from a single energetic electron.  These properties make it ideal for producing an accurate u-wire gain measurement.  Using the resulting gain corrections, individual u-wire pulse magnitudes are corrected based on their gain, and the charge cluster energy measurements are corrected according to the corrections on their bundled u-wire pulses~\cite{EnergyDocumentRun2a}.

There are also two Z-dependent energy corrections which must be applied to clusters.  The first accounts for charge attenuation due to the imperfect purity of the xenon.  As electrons drift, they may attach to electronegative impurities such as water, oxygen, and methane; the exact nature of the electronegative impurities in EXO-200 are not known.  The level of impurities in the xenon are time-dependent due to the strong effects which the xenon pump speed and periodic xenon feeds may have.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{LongThPurityMeasurement.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The purity charge correction is measured by fitting the $2615$-keV $^{208}$Tl gamma line as a function of Z-position~\cite{EnergyDocumentRun2ab}.}
\label{fig:ThPurityMeasurement}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

To extract the purity correction, the dataset is divided into time windows when the pump speed was constant and no feeds occurred; the purity is assumed to be constant during these time intervals.  It is then possible to combine all thorium source data during these time windows and fit for the $2615$-keV single-site $^{208}$Tl gamma line in Z-bins ranging from the North to the South anode.  An illustration of the ionization peak position versus Z is shown in figure~\ref{fig:ThPurityMeasurement}.  Charge attenuation is exponential with a typical $e$-fold attenuation occurring in $4-5$ ms drift time; the purity correction to charge energy is typically $2-3\%$ for charge deposits at the cathode~\cite{EnergyDocumentRun2ab}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{GridCorrectionVsZ.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The expected grid correction of a u-wire as a function of $Z$-position.  In this plot, $Z = 6$ mm represents the position of the u-wire plane and $Z = 12$ mm is the position of the v-wire plane~\cite{EnergyDocumentRun2a}.}
\label{fig:GridCorrectionVsZ}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The other Z-dependent charge correction is from the shielding grid.  Section~\ref{sec:DetectorReadout} described the process by which both electron clouds and ionized xenon will induce signals on the wires.  The electron cloud drifts rapidly, and its pulse is observable; the ionized xenon will drift quite slowly, and will not produce an observable pulse in our electronics.  However, the ionized xenon will counteract the induced current pulse from the drifting electrons by holding some electrons on the wire; the degree to which they are able to influence the observed current signal depends on the weight potential of the wire evaluated at the position of the xenon ions.  The weight potential of a u-wire is shown as a function of Z in figure~\ref{fig:GridCorrectionVsZ}, and we can see that inside the v-wire plane this correction is expected to remain below $1\%$~\cite{EnergyDocumentRun2a}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{ResidualChargeZBias.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{After applying all charge corrections, we find that the $2615$-keV $^{208}$Tl ionization peak is observed at the expected location for single-wire events; however, for two-wire events a residual bias is observed which also exhibits Z-dependence~\cite{EnergyDocumentRun2ab}.}
\label{fig:ResidualChargeZBias}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

These are all of the known corrections to the charge energy.  However, we find that after applying them, there remains a residual z-dependence in the charge calibration.  Figure~\ref{fig:ResidualChargeZBias} shows that although the $2615$-keV $^{208}$Tl ionization peak has been properly corrected for clusters which deposit on only one u-wire, for clusters which are split between two u-wires there is a residual offset in peak position of roughly $1\%$ and an additional z-dependent effect of up to roughly $0.3\%$.  These effects are not presently understood; however, they are believed to induce a Z-dependent bias effect in the lightmap which is used for scintillation denoising, which is believed to cause the scintillation z-bias described in section~\ref{sec:ResultEnergyLight}~\cite{EnergyDocumentRun2ab}.

\subsection{Light Corrections}\label{sec:ResultEnergyLight}

Although scintillation measurements are made from fitting the magnitudes of the scintillation pulses found in reconstruction, as described in section~\ref{sec:ReconPulseFitting}, these scintillation measurements are superceded by the denoised scintillation energies estimated using the denoising method of chapter~\ref{ch:DenoisingTheory}.  This subsection will describe evaluations of the success of that denoising effort and subsequent corrections made to the denoised scintillation energy.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{DenoisedScintillatonRelativeRZBias.png}
\includegraphics[keepaspectratio=true,width=\textwidth]{DenoisedScintillatonZBias.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{A residual calibration offset and Z-dependent behavior is observed in the denoised scintillation measurements.  The top plots show the relative peak position of the scintillation-only $2615$-keV $^{208}$Tl peak (one-wire and two-wire) and average alpha energy from $^{222}$Rn versus Z (top left) and R (top right).  The bottom plot shows the absolute denoised scintillation energy from the $2615$-keV $^{208}$Tl peak for one and two wires, with the measured correction function overlaid~\cite{EnergyDocumentRun2ab}.}
\label{fig:ResidualLightZBias}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

When the denoising algorithm operates properly, it should be constrained to produce scintillation energy measurements calibrated in keV.  This calibration is accomplished by the lightmap, which retains information about the magnitude of pulses produced by $2615$-keV energy deposits.  However, in practice it is seen that the denoised scintillation measurements are not properly calibrated; this can be seen in figure~\ref{fig:ResidualLightZBias}, where it is clear that events of all types (alpha decays in low-background data and both one-wire and two-wire single-site gamma decays from an external source) measure a scintillation energy which is systematically too low and displays significant Z-dependence.  The thorium data, which provides more the best measurement statistics, shows a mis-calibration which ranges from $2.5\%$ near the anodes to $6.5\%$ at the cathode~\cite{EnergyDocumentRun2ab}.

The cause for the discrepancy in scintillation energy measurements is not currently known.  One hypothesis is that the discrepancy in two-wire charge cluster measurements described in section~\ref{sec:ResultEnergyCharge} may lead to inaccurate event selection in the generation of the lightmap described in section~\ref{sec:LightmapEventSelection}; poor event selection would lead to a systematic error in the lightmap which could depend on Z, and would be capable of creating the observed bias.  Attempts to address this Z-bias within the generation of the lightmap or investigate alternative explanations for its origin have so far been unsuccessful, but these investigations continue, and it is hoped that this issue can be fixed for the next analysis.

In the meantime, for this analysis an empirical correction of the discrepancy is applied.  The scintillation correction function takes the form:
\begin{equation}
E_{corr} = E_{meas}\cdot \begin{cases}
\left[ (c_{-}) + (a_{-})\cdot |Z|^{b_{-}} \right]^{-1} & \text{if } Z < 0\\
\left[ (c_{+}) + (a_{+})\cdot |Z|^{b_{+}} \right]^{-1} & \text{if } Z > 0,
\end{cases}\end{equation}
where we measure from data $c_{-} = 0.9380$, $c_{+} = 0.9355$, $b_{-} = 1.71$, $b_{+} = 2.00$, $a_{-} = 0.69$, $a_{+} = 1.3$, and $Z$ is in units of meters.  The bottom plot of figure~\ref{fig:ResidualLightZBias} shows this empirical correction overlaid on one-wire and two-wire thorium source data, demonstrating the function agrees well with the observed Z-dependent offset~\cite{EnergyDocumentRun2ab}.

\subsection{Rotated Energy Calibration}\label{sec:RotatedEnergyCalibration}

From the corrected scintillation and charge energy measurements, it is necessary to form a calibrated rotated energy measurement which combines them.  This combination will take advantage of anticorrelation between charge and light to achieve an optimal energy resolution.  To accomplish these measurements, it is necessary to use multiple calibration points; we use the $2615$-keV gamma line from regularly-taken thorium source runs to measure time-dependent calibration parameters, and the $1173$-keV and $1332$-keV gamma lines from cobalt source runs and $661$-keV gamma line from cesium source runs to characterize the energy depence of calibrations.  The linear combination of scintillation and charge will be selected to optimize energy resolution; the precise measurement of energy resolution will be described in section~\ref{sec:RotatedEnergyResMeasurement}, but for this purpose we will simply seek to make energy peaks as narrow as possible.

The calibrated rotated energy $E$ is computed by a calibration function of the form:
\begin{align}
E_{rot} &= E_S \cdot sin(\theta(t)) + E_C \cdot cos(\theta(t))\\
E_{ratio} &= \frac{E_{rot}}{E^{th}_{rot}(t)}\\
E &= \left( E_{thorium} - E_{bias} \right) \cdot \left( p_0 + p_1 E_{ratio} + p_2 E_{ratio}^2\right),
\end{align}
where $E_C$ and $E_S$ are the measured charge and scintillation energies (with corrections applied as described in sections~\ref{sec:ResultEnergyCharge} and \ref{sec:ResultEnergyLight}), $\theta(t)$ is a time-dependent rotation angle measured to optimize the fractional energy resolution of the thorium gamma line, $E^{th}_{rot}(t)$ is the time-dependent location of the thorium peak in the spectrum of $E_{rot}$, $E_{thorium}$ is the true thorium peak position equal to $2615$ keV, and $E_{bias}$, $p_0$, $p_1$, and $p_2$ are time-independent calibration parameters.  Entirely separate calibration parameters are obtained for single-site and multi-site events.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{RotationTh2D_withCalibration.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Corrected scintillation versus charge energy are shown for a thorium run.  The projection angle for the rotated energy measurement is illustrated at the $2615$-keV peak; the projection onto a calibrated axis is shown.  Figure provided by Liangjian Wen.}
\label{fig:LiangjianAnticorrelatedEnergy}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{RotationTh2D_ImprovementInResolution.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Charge-only, scintillation-only, and rotated energy spectra are shown from a thorium source run; the significant improvement in energy resolution with the rotated energy measurement is apparent, as are low-energy features which are washed out in the lower-resolution spectra.  The dotted red line indicates the diagonal cut described in section~\ref{sec:ResultFitting}.  Figure provided by Liangjian Wen.}
\label{fig:LiangjianRotatedSpectrumImprovement}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The result of this calibration is shown in figure~\ref{fig:LiangjianAnticorrelatedEnergy}.  We can see here that a projection is performed from the two-dimensional charge-light spectrum along an axis that minimizes the width of the thorium gamma line.  Figure~\ref{fig:LiangjianRotatedSpectrumImprovement} demonstrates the spectra we obtain from projections only in scintillation, only in ionization, and using the optimal linear combination we have described; the improvement in resolution from using an appropriate linear combination of scintillation and ionization is apparent, as the peak at $2615$ keV becomes sharper and additional low-energy features of the spectrum become apparent which were washed out with worse resolution.  The next section will describe the process of measuring the energy resolution.

\subsection{Measurement of Rotated Energy Resolution}\label{sec:RotatedEnergyResMeasurement}

Along with the energy calibration, we must also measure the energy resolution of the rotated spectrum.  The resolution is assumed to be energy-dependent; if a monoenergetic deposit is made in the liquid xenon, we hypothesize that the observed spectrum should be gaussian and define the energy resolution at that energy to be the value of $\sigma$.  We can parametrize the dependence of $\sigma$ on energy as:
\begin{equation}
\sigma(E) = \sqrt{p_0^2 E + p_1^2 + p_2^2 E^2},
\end{equation}
where typically we interpret the parameters $p_1$ to come from electronic noise (which contributes a smearing independent of energy), $p_0$ to come from statistical fluctuations in the number of photons or electrons observed (which follows a Poisson distribution), and $p_2$ to come from mis-calibration of components of the detector (whose effect does not diminish with increased energy)~\cite{knoll2000radiation}.  We also often find it useful to refer to the relative or fractional energy resolution, defined as:
\begin{equation}
\sigma(E)/E = \sqrt{p_0^2/E + p_1^2/E^2 + p_2^2}.
\end{equation}
The resolutions of single-site and multi-site events differ, so there are separate resolution functions measured for both classes of events.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline \multirow{2}{*}{Parameter} & \multicolumn{2}{|c|}{Undenoised} & \multicolumn{2}{|c|}{Denoised}\\
\cline{2-5}& SS & MS & SS & MS \\
\hline $p_0$ & 0 & 0 & 0.628 & 0.602 \\
\hline $p_1$ & 39.8 & 43.0 & 20.8 & 25.8 \\
\hline $p_2$ & 0.0107 & 0.0096 & 0.0011 & 0.0040 \\ \hline
\end{tabular}
\end{center}
\caption{Time-averaged resolution parameters for denoised and undenoised data.  For undenoised data, measurements were consistent with $p_0 = 0$, so this parameter was not used in the analysis of undenoised data~\cite{AverageEnergyResolutionDocument}.}
\label{tab:ResolutionFunctions}
\end{table}

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.95\textwidth]{ResolutionFunctionComparison_absolute.pdf}
\includegraphics[keepaspectratio=true,width=.95\textwidth]{ResolutionFunctionComparison_relative.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The resolution functions (top) and relative resolution functions (bottom) are compared for denoised and undenoised data.}
\label{fig:ResolutionFunctionComparison}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The resolution functions are measured from the same set of calibration gamma lines (at $661$, $1173$, $1332$, and $2615$ keV) that are used for the energy calibration.  The resolution parameters are time-dependent; average resolution parameters are computed by weighting the resolutions observed in different time windows by the fraction of EXO-200 low-background data taken within that window.  The resulting resolution parameters are listed in table~\ref{tab:ResolutionFunctions}.  Figure~\ref{fig:ResolutionFunctionComparison} compares the denoised and undenoised resolution functions.  The strongest effect is the reduction of $p_1$ by roughly a factor of two in the denoised data, consistent with the interpretation that denoising improves the energy resolution primarily by reducing the impact of electronic noise.

We note that $p_0$ shifts from negligible to measurable when denoising is applied.  It is suggestive to interpret this as meaning denoising increases noise due to statistical fluctuations in the number of collected photons.  The undenoised scintillation measurements are made from summed waveforms, so it uses the full photon statistics of all channels; as a result, we can expect that $p_0$ is as small as possible in the undenoised data.  By contrast, denoising may choose to concentrate its scintillation measurement on a smaller number of channels, reducing electronic noise at a cost in greater statistical fluctuations.  Furthermore, we recall in section~\ref{sec:DenoisingInPractice} that the current analysis mistakenly underestimates Poissonian noise produced inside the APDs, and this would manifest itself by subjecting the scintillation estimate to even more statistical fluctuations than would be ideal.  It is therefore possible that the increase in $p_0$ may demonstrate denoising has sacrificed photon collection efficiency for the purpose of reducing electronic noise.

The parameter $p_2$ seems to be significantly reduced by denoising.  This can be interpreted as demonstrating a better understanding of APD gain in denoised scintillation measurements.  In the undenoised scintillation measurements, all APD channels are summed together without regard to the different gains of each APD channel; we can expect this to lead to a significant variation in gain depending on which APD channel was nearest to an energy deposit, which would appear as a significant contribution to $p_2$.  On the other hand, the denoised scintillation measurement is build around a channel-by-channel APD lightmap which contains information about the gain of every APD channel, which could reduce the impact of local variations in gain and, consequently, the magnitude of $p_2$.

It should be noted that, although these interpretations of $p_0$, $p_1$, and $p_2$ are highly suggestive, the naive interpretations of their meaning may not be fully applicable to the rotated energy measurements.  Other energy-dependent quantities may not be fully understood; for example, it has been suggested that the rotation angle may be energy-dependent, and our failure to include this dependence may be reflected by an energy dependence in the resolution function~\cite{EnergyDocumentRun2a}.  Also, the error bands in figure~\ref{fig:ResolutionFunctionComparison} are still somewhat broad, and more accurate resolution measurements or resolution measurements from a wider energy range would be useful to contrain the values of these parameters better.  Further investigation will be required to verify that the scintillation energy measurement accuracy is still the dominant contributor to energy resolution at all energy scales.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{RotatedReso_denoised_compVsTime.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Time dependence of the single-site energy resolution at $2615$ keV with denoised (blue) and undenoised (red) scintillation.  The denoised energy resolution at the $\beta\beta 0\nu$ $Q$-value is estimated (open square) from the time-dependent energy resolution measured with thorium data combined with the time-independent shape of the average resolution function~\cite{NewEXObb0nPaper_2014}.}
\label{fig:ResolutionTimeDependenceComparison}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

Although we have identified the energy resolution functions averaged over our entire dataset, it is also instructive to study the time-dependence of the energy resolution.  We cannot produce a full resolution function which is time-dependent because the cesium and cobalt source calibrations were only occasionally taken, but it is possible to study the time dependence of the resolution at the $2615$-keV $^{208}$Tl line of the thorium source which was deployed much more frequently.  This resolution is shown as a function of time in figure~\ref{fig:ResolutionTimeDependenceComparison}.  We can see that the energy resolution is improved for all time periods.  We can also see that the variations in resolution are much smaller in denoised than undenoised energy; in particular, the period of worse resolution between October 2012 and June 2013 is brought closer to the resolution during the rest of the data run by the denoising process.  Time fluctuations in the undenoised energy resolution are attributed to fluctuations in the electronic noise on the APDs, and this flattening-out of the resolution's time dependence after denoising shows that denoising is capable of removing the impact of this time-varying noise~\cite{NewEXObb0nPaper_2014}.

Besides the energy resolution measurements from source data described above, it is possible to possible to identify gamma lines in the low-background data.  These are currently used as a cross-check on the source calibrations rather than additional input into the calibration procedure.  They can serve the additional purpose of verifying that calibrations from a wide range of times can be combined to produce coherent peaks; they can also verify that the average resolutions extracted from source data are accurate because low-background sources are inherently averaged over the duration of the data run.  Two energy lines from the low-background spectrum have been identified as particularly useful.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{neutronHydrogen_ss_denoised.pdf}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{neutronHydrogen_ms_denoised.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The single-site (left) and multi-site (right) gamma line at $2.2$ MeV from neutron capture on hydrogen can be used as a low-background cross-check on the resolution calibration~\cite{EnergyDocumentRun2ab}.}
\label{fig:neutronHydrogenCheck}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

One gamma line in the low-background data comes from neutron capture on hydrogen in the HFE refrigerant.  Such events are generally vetoed by the coincidence cuts, primarily the veto panels, so their impact on the low-background data is suppressed; conversely, it is possible to study only events which occur in coincidence with muon detection by the veto panels, thereby enriching our dataset in neutron capture spectra.  Neutron capture on hydrogen can emit a $2.2$ MeV gamma; figure~\ref{fig:neutronHydrogenCheck} shows the spectra from this source~\cite{NeutronCaptureGammas,EnergyDocumentRun2ab}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{EXO_Workspace_Run2abc_K40_ss_1250keVto1650keV.pdf}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{EXO_Workspace_Run2abc_K40_ms_1250keVto1650keV.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The low-background potassium peak at $1461$ keV is used as a cross-check on the single-site (left) and multi-site (right) energy calibrations.  Fits also include the nearby $1332$-keV cobalt peak~\cite{EnergyDocumentRun2ab}.}
\label{fig:PotassiumPeakCheck}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The other gamma line which has been used as a cross-check is the $1461$-keV $^{40}$K gamma line.  $^{40}$K is expected to occur in the copper of the TPC at some low level, and this gamma line is readily visible in the multi-site spectrum of low-background data.  The improved energy resolution of denoised data allows us to also identify the $^{40}$K single-site peak over the $\beta\beta 2\nu$ background, which was not previously possible.  These peaks are shown in figure~\ref{fig:PotassiumPeakCheck}.

\section{Fitting}\label{sec:ResultFitting}

After the energy calibrations have been performed, it is possible to produce a set of data with calibrated energy measurements.  We will now describe the selection of usable events from this dataset, the creation of corresponding background probability distribution functions (PDFs), the fitting of data to extract the best-fit number of $\beta\beta 0\nu$ events in the dataset, and the association of errors with that number.  Results from the current analysis will be demonstrated along the way, leading to a limit for $\beta\beta 0\nu$ decay.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{FidVolDiagram_Dissertation.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Here the X-Y orthogonal coordinates are shown along with the U-V coordinates that run orthogonal to the wire planes.  The X-Y fiducial cuts applied to the data (dashed hexagon) do not include the entire active volume (solid hexagon); for the $\beta\beta 0\nu$ search we find that aggressive fiducial cuts optimize our sensitivity, so very little active xenon is left unused.}
\label{fig:FidVolDiagram}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{SS_Spectrum_noVetoCuts.png}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Vetoing events coincident with muons can reduce our $1\sigma$ and $2\sigma$ event counts.  Figure provided by David Auty.}
\label{fig:Spectrum_noVetoCuts}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

It is necessary to perform a number of cuts on the dataset before it is usable.  These cuts are intended to reject events which may not agree with the model used to generate background PDFs.  These cuts include:~\cite{FittingDocument}
\begin{itemize}
\item A fiducial volume cut to remove events with any energy deposits outside of a pre-defined volume.  We choose to define a fiducial volume defined in Z by $10 \text{mm} < |Z| < 182 \text{mm}$ and in the X-Y plane as the area within a hexagon with an apothem of $162$ mm, as illustrated in figure~\ref{fig:FidVolDiagram}.  These cuts allow us to ignore regions near the detector walls or v-wire plane where the electic fields may not behave as expected.  They also ensure that beta or alpha backgrounds from the walls of the detector do not penetrate into our fiducial volume, allowing us to simplify the set of backgrounds modeled and included in fits.
\item A ``diagonal'' cut to remove events with excessive scintillation relative to ionization.  Alpha decays produce a light-to-charge ratio much higher than beta and gamma decays, as described in section~\ref{sec:DetectorActiveBackgroundRejection}, so excluding events with excessive light permits us to neglect alpha-decay backgrounds dissolved in the xenon.  Events with excessive light can also be caused by decays outside of our active xenon, where charge is not collected efficiently; these events will be difficult to calibrate, and should not be included.  Both are excluded by a cut on the light-to-charge ratio, as shown in figure~\ref{fig:LiangjianAnticorrelatedEnergy}.
\item A coincidence cut to remove events occurring near in time to a passing muon.  Muons and their spallation products can produce a wide range of backgrounds which is difficult to model; most of these backgrounds are quite short-lived, so a coincidence cut can provide an effective means of reducing them.  Currently we cut all events occurring within one minute of a muon observed passing directly through the TPC and within $25$ ms after a muon observed passing through the veto panels.  Furthermore, events which occur within one second of each other are both cut based on the expectation that the events were probably correlated and therefore more likely to be some form of background.  Figure~\ref{fig:Spectrum_noVetoCuts} demonstrates the significant reduction in data rate which is achieved by applying this coincidence cut.
\end{itemize}

Fits are then performed using the profile-likelihood method, as described in~\cite{ProfileLikelihood}.  Each data point is assigned a likelihood $\text{pdf}(x)$ equal to the evaluation of the normalized pdf at that point; the test statistic of interest is the negative log-likelihood, defined by:
\begin{equation}
\text{NLL} = -\sum_i ln\left[\text{pdf}(x_i)\right] + \text{constraints}.
\end{equation}
Contraints for us will consist of gaussian penalty terms applied to parameters whose value is constrained by independent studies external to the fit.  The best fit will be the one which minimizes the NLL.  Fit errors on parameters can be extracted by observing how the NLL changes when those parameters are forced away from their best-fit values.  In cases where the best-fit value of a parameter is near its boundary, these errors may not provide a good estimate of the true confidence interval or limits, so when this situation occurs for a parameter of significant interest (specifically, the rate of $\beta\beta 0\nu$, which fits near to zero) we can verify that the confidence limit is not overstated using toy monte carlo studies~\cite{FittingDocument}.

The fits make use of three observables.  The first, event energy, has been discussed at length in section~\ref{sec:RotatedEnergyCalibration}.  The second is the event classification as single-site or multi-site; we define an event as single-site if reconstruction located only one cluster, and that cluster deposited charge on no more than two u-wire channels~\cite{FittingDocument}.

The third observable is called standoff, and it attempts to capture the nearness of an event to the TPC walls.  Most backgrounds are external, and can be expected to deposit energy preferentially near the walls of the TPC; by contrast, $\beta\beta 0\nu$ and $\beta\beta 2\nu$ decay come from the xenon, and should be distributed uniformly through the detector.  We define the standoff distance to be the shortest distance between some deposit cluster and either the v-wire planes or the teflon walls.  Ideally, it would be possible to capture finer position information by using all three position coordinates as observables; however, in practice it is difficult to construct a pdf with sufficient statistics in so many dimensions, so the standoff observable has been constructed to capture the most interesting distances in a single observable.  The exact definition is not too important provided it is modeled properly by the simulation; figure~\ref{fig:RaSourceMCComparison} demonstrates that standoff distance is simulated well for all but the smallest values (closest to the TPC walls or anode), and this is accounted for as a shape systematic~\cite{FittingDocument}.

Fit parameters include the number of counts observed from each pdf and the fraction of counts from each pdf which populate the single-site spectrum; the latter is constrained based on the agreement between simulated and observed single-site fraction from source runs, but is allowed to float within those uncertainties.  The pdfs which are included are:
\begin{itemize}
\item $^{136}$Xe $\beta\beta 2\nu$ decay.
\item $^{136}$Xe $\beta\beta 0\nu$ decay.
\item $^{232}$Th from the TPC vessel.
\item $^{232}$Th from distant sources (HFE refrigerant or cryostat).
\item $^{238}$U from the TPC vessel.
\item $^{214}$Bi from the TPC cathode ($^{238}$U chain).
\item $^{214}$Bi from the air gap inside the lead wall ($^{238}$U chain).
\item $^{214}$Pb dissolved in active xenon ($^{238}$U chain).
\item $^{222}$Rn dissolved in inactive xenon ($^{238}$U chain).
\item $^{60}$Co from the TPC vessel.
\item $^{40}$K from the TPC vessel.
\item $^{65}$Zn from the TPC vessel.
\item $^{54}$Mn from the TPC vessel.
\item $^{137}$Xe from neutron capture on $^{136}$Xe.
\item $^{135}$Xe from neutron capture on $^{134}$Xe.
\item Various neutron captures on xenon, copper, and hydrogen.
\end{itemize}
Similar pdfs from different locations are included because the sharpness and relative intensity of peaks can be affected by intervening material.  The rate of $^{214}$Bi from the air gap is constrained by measurements of radon levels in air, and the rates of $^{214}$Bi from the TPC cathode, $^{214}$Pb dissolved in active xenon, and $^{222}$Rn dissolved in inactive xenon are jointly constrained by independent searches for $^{214}$Bi-$^{214}$Po rates.  The overall rate of neutron captures is allowed to float, but the relative intensities of the captures on xenon, copper, and hydrogen are constrained by simulation, and the single-site fraction is fixed because there are not expected to be sufficient statistics in the single-site spectrum to constrain it~\cite{FittingDocument}.

In addition to the magnitude and single-site fraction parameters of the fit, there are a few global parameters which are also allowed to float.  There is a normalization constant which is allowed to float within constraints and accounts for uncertainty in the fiducial volume or detection efficiency.  Constraints on this term come from comparisons between the predicted and observed event rate from sources.  Denoising does not currently operate on events whose waveforms are shorter than the standard length of $2048 \mu$s; since these occur more in source data than low-background data, an additional correction to the simulated event rate must be generated, and denoising may in this way worsen the event rate agreement in source data even though it is not expected to show a significant effect in low-background data.  Additionally, the normalization constants for gamma and beta particles are allowed to float separately because their reconstruction efficiency may be different.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,height=3in]{NLLProfile_bscale_scan.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The fit provides constraints on the beta-scale.  Figure provided by Liangjian Wen.}
\label{fig:BetaScaleNLL}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

Furthermore, it is expected that betas and gammas may need to be calibrated separately.  To a first approximation, gammas deposit energy in the detector by exciting electrons, so these processes should be quite similar.  However, to account for possible differences, we define a beta-scale $\beta$ which quantifies a linear calibration offset between beta and gamma deposits:
\begin{equation}
E_\beta = \beta E_\gamma.
\end{equation}
The beta scale is permitted to float, and is constrained by the fit; figure~\ref{fig:BetaScaleNLL} shows the NLL profile with various choices of beta scale, and demonstrates that it is possible to constrain the beta scale to a value close to $1$ using only the spectral information in the fit.  Uncertainty on the beta scale is incorporated by permitting it to float freely.  We note that currently the beta scale is implemented by shifting beta-like pdf components ($\beta\beta 0\nu$, $\beta\beta 2\nu$, $^{135}$Xe, $^{137}$Xe) relative to the other components which are gamma-like; this ignores the detail that some events may contain clusters generated through both beta and gamma interactions, such as pair production from energetic gammas.  These issues will be addressed in a future analysis, but for now the beta scale is assigned only to decays whose primary particle is a beta~\cite{FittingDocument}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{Energy_BestFit_frompaper.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Energy specra with best-fit pdfs for single-site (top) and multi-site (bottom).  Residuals between data and the combined pdfs are shown below the spectra.  The last bin of the spectra is an overflow bin.  The $2\sigma$ region of interest around the $Q$-value is shown on the single-site spectrum.  Insets for single-site and multi-site zoom around the $Q$-value.  The simultaneous standoff-distance fit is not shown~\cite{NewEXObb0nPaper_2014}.}
\label{fig:BestFitEnergyFromPaper}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

When all of this is done, it is possible to obtain a fit to the low-background spectrum; the maximum-likelihood fit is shown in figure~\ref{fig:BestFitEnergyFromPaper}.  The best-fit expected background in the $2\sigma$ single-site energy window around our $Q$-value is $31.1 \pm 1.8\text{(stat)} \pm 3.3\text{(sys)}$ counts, corresponding to a background rate of $(1.7 \pm 0.2)\cdot 10^{-3} \text{keV}^{-1} \text{kg}^{1} \text{yr}^{1}$.  The backgrounds in this $2\sigma$ energy window come from the thorium decay chain ($51\%$), uranium decay chain ($26\%$), and $^{137}$Xe ($23\%$), with other contributions negligible.  The observed event count in the $2\sigma$ single-site energy window is $39$ counts~\cite{NewEXObb0nPaper_2014}.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{ROIdistribution_XY.eps}
\includegraphics[keepaspectratio=true,width=.49\textwidth]{ROIdistribution_RZ.eps}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{The positions of events in the $1\sigma$ region of interest (red) and wider energy range $2325-2550$ keV (black) are shown projected onto their X-Y (left) and $R^2$-Z (right) coordinates.  Lines indicate the fiducial volume, with the dotted lines indicating the fiducial cut away from the cathod that is not used as a component to the standoff distance observable. Figure provided by Dave Moore.}
\label{fig:ROIEventPositionDistribution}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

(We remind the reader at this point that the quoted limit on $\beta\beta 0\nu$ is not simply based on this comparison of counts, but based on the full set of energy and standoff-distance spectral information.  Representative position information can be seen in figure~\ref{fig:ROIEventPositionDistribution}.  However, the simple comparison of counts can be instructive as an easier-to-understand approximation to the following limits.)

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,height=3in]{0nubb_profile_frompaper.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Profile scan of negative log-likelihood as a function of the number of fit $\beta\beta 0\nu$ counts~\cite{NewEXObb0nPaper_2014}.}
\label{fig:bb0nProfileFromPaper}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The likelihood profile for the total number of $\beta\beta 0\nu$ counts occurring in our detector during the lifetime of the experiment is shown in figure~\ref{fig:bb0nProfileFromPaper}.  Based on the profile, we can see that a $1\sigma$ confidence interval for this parameter would exclude the null hypothesis of zero $\beta\beta 0\nu$ counts; however, only $1\sigma$ exclusion of the null hypothesis is far too weak to make an inference of a non-zero signal.  Instead, we follow the conventions of the field and quote a $90\%$-confidence upper limit of $24$ total counts from $\beta\beta 0\nu$.  This corresponds to a $90\%$-confidence lower limit on the halflife of $^{136}$Xe decay by the $\beta\beta 0\nu$ mode of $1.1 \cdot 10^{25}$ years~\cite{NewEXObb0nPaper_2014}.

\section{Results and Physics Reach}\label{sec:ResultResults}

We have stated the half-life limit on $\beta\beta 0\nu$ decay of $^{136}$Xe, which is the main result of the present analysis.  We shall now continue to put this result in context with the expected sensitivity of the experiment and compare it to the limits and sensitivities achieved by other leading $\beta\beta 0\nu$ experiments.

To place the limit derived with the current dataset in context with the overall strength of the experiment, we generate a number of toy monte carlo datasets.  We assume that our best-fit backgrounds are fully accurate and that the null hypothesis (no $\beta\beta 0\nu$) is true; we then use this best-fit background as our pdf and generate toy datasets.  These can be used to study the ``typical'' results our detector would have produced if the same experiment could be repeated many times.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=.9\textwidth]{toy_mc_sensitivity.pdf}
\includegraphics[keepaspectratio=true,width=.9\textwidth]{toy_mc_significance.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{Assuming that the background measurements of EXO-200 from this analysis are accurate, toy datasets are simulated.  The top plot shows the probability distribution of our $90\%$ confidence limit; we find that our median upper limit is 14.04 counts attributed to $\beta\beta 0\nu$, compared to our observed upper limit of 23.92 counts.  The bottom plot shows the probability distribution of our ability to reject the null hypothesis (no $\beta\beta 0\nu$) based on the negative log-likelihood; we find that the present dataset rejects the null hypothesis with probability less than $90\%$.  Figures provided by Ryan Killick.}
\label{fig:ToyMCSensitivitySelectivity}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The resulting distributions are shown in figure~\ref{fig:ToyMCSensitivitySelectivity}.  The top plot shows the distribution of $90\%$-confidence upper limits on the number of $\beta\beta 0\nu$ counts; we can see that typical datasets from the EXO-200 experiment result in limits ranging from 8 to 22 counts.  The median limit, is 14 counts; this value is called the median sensitivity, and it illustrates the overall strength of an experiment.  We contrast this with the observed limit of 24 counts; as is apparent from the distribution, this limit is consistent with an unlucky upward fluctuation in the backgrounds observed by EXO-200.

We saw in the NLL profile on the total number of $\beta\beta 0\nu$ counts in figure~\ref{fig:bb0nProfileFromPaper} that the NLL for the hypothesis of no $\beta\beta 0\nu$ counts is about 1.1 higher than for the best fit; this quantity reflects how well we can reject the null hypothesis in favor of the best fit.  To put this in context, we also observe the distribution of this value from toy monte carlo datasets in the bottom plot of figure~\ref{fig:ToyMCSensitivitySelectivity}.  Here, we see that $10.8\%$ of identical experiments will yield an NLL profile which is at least 1.1 worse for no $\beta\beta 0\nu$ than for the best fits even though no $\beta\beta 0\nu$ is simulated.  We can conclude that this difference in NLL is not strong enough to reject the null hypothesis with high confidence.

\begin{figure}
\begin{center}
\includegraphics[keepaspectratio=true,width=\textwidth]{MatplotLibSensitivity.pdf}
\end{center}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{quote}
\caption{$\beta\beta 0\nu$ constraints are shown from $^{136}$Xe (horizontal) and $^{76}$Ge (vertical).  The four diagonal lines represent four recent matrix element computations \cite{PhysRevLett.105.252503,Menndez2009139,PhysRevC.87.014315,PhysRevC.87.045501} and one recent phase-space factor~\cite{PhysRevC.85.034316} relating the two half-lives; tick marks along the diagonal represent the corresponding mass $\left<m_{\beta\beta}\right>$.  $90\%$ confidence limits and median $90\%$ sensitivities are shown for the KamLand-Zen~\cite{PhysRevLett.110.062502}, GERDA~\cite{PhysRevLett.111.122503}, and EXO-200 experiments~\cite{NewEXObb0nPaper_2014}; the claimed 2006 discovery by Klapdor-Kleingrothaus and Krivosheina is shown as a $1\sigma$ confidence interval~\cite{Klapdor}.}
\label{fig:MatplotlibSensitivity}
\end{quote}
\end{figure}
\renewcommand{\baselinestretch}{2}
\small\normalsize

The wide spread of limits produced in toy monte carlo studies illustrates the significant role that luck plays in the limits set by low-statistics experiments such as EXO-200.  To compare EXO-200 to other experiments, it is useful to compare both the limits and the median sensitivities (median $90\%$-confidence limits).  This is done in figure~\ref{fig:MatplotlibSensitivity}.  Vertical lines correspond to the reported median sensitivities of the two leading $^{136}$Xe experiments, KamLAND-Zen and EXO-200; right-pointing arrows indicate the actual limits set by each.  For both experiments we can see that statistical fluctuations in the background account for factor-of-two differences between median sensitivity and observed limits; EXO-200 claims to be the most sensitive $^{136}$Xe experiment, but KamLAND-Zen currently reports the strongest limit.  Both experiments intend to continue running, and EXO-200 hopes that their next dataset is atypical in the other way.

Nuclear matrix elements are difficult to compute, as described in \textcolor{red}{the section I need to expand}.  However, figure~\ref{fig:MatplotlibSensitivity} illustrated four diagonal lines corresponding to four different modern nuclear matrix calculations, and uses these values to relate the half-lives of $^{136}$Xe and $^{76}$Ge.  The current median sensitivity and limit on the $\beta\beta 0\nu$ half-life of $^{76}$Ge from GERDA are shown as a horizontal line and vertical arrow, respectively.  This is contrasted with the 2006 claimed discovery by Klapdor-Kleingrothaus and Krivosheina of $\beta\beta 0\nu$ decay, which is not excluded by the more recent GERDA limit.

Furthermore, the nuclear matrix elements permit us to translate half-life lower limits into upper limits on the effective Majorana neutrino mass $\left<m_{\beta\beta}\right>$.  The relation is indicated, for each choice of nuclear matrix elements, by tick marks on the corresponding diagonal line.  We can see that the GERDA and EXO-200 experiments are of similar strengths, depending most on the choice nuclear matrix elements; KamLAND-Zen is comparable in strength to GERDA for the RQRPA nuclear matrix elements, and stronger for all other choices in this plot.  The $90\%$-confidence mass limit quoted by EXO-200 is $\left<m_{\beta\beta}\right> < 190-450$ meV, depending on the choice of matrix elements.  Recalling figure~\ref{fig:NeutrinoMassBounds}, this corresponds to a $90\%$-confidence limit on the mass of the lightest neutrino eigenstate of $m_{min} < 0.691.63$ eV under the most conservative choice of Majorana phases~\cite{NewEXObb0nPaper_2014}.

\section{Comparison to Results without Denoising}\label{sec:ResultComparison}

\textcolor{red}{I should do this -- but need to finalize plots, which still needs a little more processing.}

\section{Summary}\label{sec:ResultSummary}
