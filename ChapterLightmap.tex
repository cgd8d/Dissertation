
\renewcommand{\thechapter}{1}
\chapter{The Lightmap}

The EXO detector was designed with roughly 450 APDs ganged into 74 channels of six to seven APDs each.  These APDs were set into the two endcaps of the cylindrical EXO detector.  Three of the APD ganged channels were disabled due to noisy components before physics data collection began; a fourth channel was disabled in February 2012 due to increasing noise in its components.  Additional stepwise changes in APD behavior come from changes to the electronics which have been implemented multiple times during the course of the experiment.

The different positions of the APDs means that given the same amount of energy deposited into the detector, a channel may show a larger or smaller signal depending on the location of the energy deposit.  Additionally, there will be time-dependent changes: gain will drift in each APD at independent rates, and stepwise changes to the APD signals comes from changes to electronics and disabling of channels.

As a result, when we attempt to characterize the APD response of our detector, it is critical to map out the time and space dependence of that response.  Early attempts to do this focused on periodic campaigns to collect large amounts of known-energy data and characterize the APD response at a moment in time.  These campaigns would generally consist of days of source data.  The strong Thorium source would be used, and an expert would be on-site to position the source in a wide range of locations, to ensure that a single campaign was independently sufficient to characterize the APD response.  The 2615-keV single-site gamma line of the Thallium 208 daughter product of Thorium 228 was isolated in offline analysis, and signals from these events were used to measure the signal magnitude from a known-energy single-position deposit in the detector.

Even with such a significant quantity of source data, statistics were found to be insufficient.  Some regions of the detector were difficult to illuminate, and signals on individual channels were small.  To simplify the problem, signals on each endcap were summed together (without gain corrections) so that the 70-71 active APD channels could be treated as two large-magnitude APD channels instead.  This increased the size of the signals; it also made the spatial dependence of the response smoother, so that a sparser distribution of data can still be sufficient to characterize the response.  This permitted the creation of an APD-plane lightmap which allowed EXO to produce its first position-dependent corrections to scintillation energy.

Although the APD-plane lightmap produced significant improvements in the energy resolution achieved by the EXO detector, inevitably it was only an incomplete characterization of the APD response.  Indeed, because for $\beta\beta0\nu$ studies we place our fiducial cut as close to the edges of the detector as possible, in certain regions of the detector our scintillation signal might be highly concentrated on one or a small number of channels.  Summing together multiple channels is, in this sense, a lossy form of compression of the data, and it is tempting to see whether we can extract better physics if it is avoided.  Thus, characterizing the APD yield on individual APD-gang channels is not in itself an important component of our analysis, but may be expected to provide a critical tool for more advanced scintillation analysis.

\section{Four-Dimensional Lightmap}

As described above, in constructing an individual-APD lightmap we face two conflicting needs: we must use as much data as possible to handle the faster spatial dependence and smaller signals expected, but if too much data is included then we run the risk of combining data taken when an APD had a different gain.  If we truly wish to use all available data, then we will need to simultaneously understand the full time-dependence of the gain.  In other words, rather than forming a small number of independently-measured three-dimensional lightmaps, we will need to measure a four-dimensional lightmap $L(x,y,z,t)$.

This may at first seem infeasible.  After all, by adding an independent time argument it appears that rather than measuring a lightmap independently from each calibration campaign, we must measure one independently for each time bin.  But we can make a key simplifying assumption that the lightmap is separable.  In a physical sense, we can assume that:
\begin{enumerate}
\item From a given position $(x,y,z)$ photons deposit on each APD channel at a constant rate.
\item Each APD channel in turn magnifies and shapes its signal with a gain which may vary in time, but does not depend on the point of origin of the photons.
\end{enumerate}
So, we demand that the lightmap have the much simpler form
\begin{equation} \label{eq:SeparableLightmap}
L(x,y,z,t) = R(x,y,z)S(t)
\end{equation}

Is this simplification fully motivated by the detector?  It must be admitted that it is not.  The first point is fairly accurate:  we must trust that the electric field and the reflectivity of detector surfaces are constant in time, but basically these conditions are assumed to hold within EXO-200.

However, each channel can draw its signal from multiple APDs, each of which has an independent time-varying gain.  Photons from a deposit may preferentially sample the gain from the closest APD within a channel, so deposits in different locations may track more closely the gain of the closest APD within a channel, and the second point may not be accurate.  Studying the impact of this effect should be a topic for future study, particularly to understand to what degree it limits the effectiveness of a lightmap; it may be relevant to nEXO planning to understand how ganging together APDs limits scintillation resolution.  Currently it is assumed to be a small effect and ignored.

So, treating equation ~\ref{eq:SeparableLightmap} as valid, one simple general scheme to find $R$ and $S$ iteratively is:
\begin{enumerate}
\item \label{item:FormingListOfHits} Compile, as well as possible, a list of single-site 2615-keV events from Thorium-228 source data.  Some Compton-scattered events will inevitably leak into this dataset, but the selection criteria should be designed to minimize this leakage while maximizing the fraction of true 2615-keV events accepted.
\item Initially, assume a constant function $S(t) = 1$ for all APD channels.
\item \label{item:ReentryForLightmapAlg} For each APD channel, estimate $R(x,y,z) = L(x,y,z,t)/S(t)$ from the set of events tabulated in step ~\ref{item:FormingListOfHits}.
\item For each APD channel, estimate $S(t) = L(x,y,z,t)/R(x,y,z)$ from the set of events tabulated in step ~\ref{item:FormingListOfHits}.
\item If convergence has not yet been reached, return to step ~\ref{item:ReentryForLightmapAlg}.
\end{enumerate}
Although this algorithm is not necessarily guaranteed to achieve convergence at all, in practice $S(t)$ is close to constant, resulting in fairly rapid convergence.

\section{Algorithm Details}

The previous section outlines a general algorithm for computing a four-dimensional lightmap from source data.  In this section I will specify the implementation choices which are made in the code currently being used.  In attempt to encourage future experimentation, alternative options will also be listed in some detail, along with some motivations for these alternatives.  Time has not permitted testing of all of these options, but it is hoped that they will be explored in the future.

\subsection{Event Selection}

The single-site Thorium-228 spectrum is well-peaked at 2615 keV, making it an excellent monoenergetic calibration line.  Our challenge is to select truly 2615-keV events as efficiently as possible, while simultaneously avoiding near-2615-keV events which may leak into the dataset.

This is an inherently iterative process.  The first lightmap had to be constructed from an ionization-only spectrum because no useful position-corrected scintillation measurement existed; the resolution of the ionization-only spectrum was quite poor, roughly $3.5\%$ (sigma) of the energy, as described in Steve Herrin's thesis, and severe compromises had to be made to keep the Compton-shoulder leakage down to acceptable levels.  That work required events to have ionization between $0.33\sigma$ and $3\sigma$ to the right of the ionization peak.~\cite{ThesisSteve}

Such a strongly asymmetric cut was chosen to avoid leakage from compton shoulder events, which it did successfully.  However, because of anticorrelation between scintillation and ionization, choosing events whose ionization had oscillated high means this cut was also preferring events whose scintillation had oscillated low, introducing a bias in the lightmap magnitude.  Additionally, the cut permits less than $37\%$ of desired events to be accepted, which is a substantial loss of efficiency that has a significant impact in certain low-statistics regions of the detector.

The current work has had the benefit of an existing position-corrected scintillation measurement, leading to a rotated energy spectrum with resolution of roughly $1.8\%$.  As a result, it is possible to accept a wider cut (in sigmas) while still keeping Compton shoulder leakage small.  We currently accept all events within $2\sigma$ of the peak, with better than $95\%$ acceptance and only small leakage.  An additional benefit is that this improved resolution allows us to use a symmetric acceptance region, avoiding the implicit bias introduced from the earlier asymmetric cut window.

Preliminary investigations have been conducted to see what impact is observed from using the denoised scintillation measurements which are the subject of this work.  Presently the improvement in resolution (to roughly $1.4\%$) has not demonstrated any significant improvement in the quality of the lightmap; however, this is a topic of continuing study which will certainly be revisited.

Beyond the cuts described above (single-site within an appropriate energy window), other basic cuts are applied to ensure only well-behaved scintillation signals are selected:
\begin{itemize}
\item Charge and light must individually lie within a reasonable range.
\item The charge-to-light ratio must not identify an event as an alpha decay.
\item The scintillation time must be well-separated from any other scintillation signals in same event frame.
\item All charge signals which are assigned to this scintillation signal must be assigned unambiguously.
\item All three position coordinates must be known.  (No fiducial cut is placed, since that would restrict the volume where the lightmap is specified.)
\end{itemize}
Many of these cuts are probably no longer necessary -- as the resolution has improved, the chances of contamination have decreased and we can be a bit less cautious about accepting events.  It is worth remembering, though, that our main analysis only accepts events with a single scintillation signal, something which we cannot afford to do.  (The statistics from strong Thorium source runs are too valuable, particularly because they are the primary source of statistics in some regions of the detector.)  As a result, it is important to maintain some caution with cuts designed to handle those events more robustly.

\subsection{Function Parametrization}

Because we are attempting to empirically measure the functions $R(x,y,z)$ and $S(t)$ from a finite dataset, we need to specify some more limited form for them to take.  All current approaches to describing $R(x,y,z)$ first bin the detector volume into three-dimensional voxels, and then define $R(x,y,z)$ to interpolate linearly between known values at the center of the voxels.  The size of these voxels must be chosen with some care; if they are too small, then low per-voxel statistics will cause the statistical errors on the signal magnitude to dominate, whereas if the voxels are too large then the spatial variation of the lightmap will not be fully captured.

In the current lightmap, the detector is binned into $1 cm^3$ voxels; the detector is easily contained within a box with sides $40 cm$ long, leading to $64,000$ voxels (many of which lie outside the detector and will be empty).  The choice of voxel size was made based on the size of an individual APD.  The APDs are roughly $2.5 cm$ in diameter, so very near the anode we would like for the size of a voxel to be much smaller than $2.5 cm$.  When this is done, much of the detector has entirely sufficient statistics per voxel; however, there are some regions of the detector with fewer than ten hits per voxel, indicating that these regions may have quite significant statistical error.

It is possible to justify a choice of larger voxels.  The APDs lie at $\pm 204 mm$ from the cathode, which means that there is more than $1 cm$ (WHAT IS IT?) between our fiducial volume and the APDs.  At this distance, the dependence of the lightmap on $x$ and $y$ is expected to be much slower than at the APD plane, indicating that perhaps $2 cm$ binning in $x$ and $y$ may be sufficient.  Additionally, $z$-dependence of the lightmap is expected to be fairly smooth throughout the detector; since we interpolate linearly between voxels, it may be possible to use a $z$-binning much coarser than $1 cm$.  This may be a topic for future investigation.

It is also worth mentioning that less intuitive voxel geometries have been tried in the past.  The older APD-plane lightmap~\cite{ThesisSteve} used a cylindrical-coordinate binning; binning in $r$ was selected to make the bin volumes roughly constant along the $r$ axis, binning in the angular coordinate was uniform, and binning in $z$ was chosen to be coarser near the anodes and finer near the cathode to reflect faster variation there.  In all cases the binning is coarser than with the current cubic voxels being used; this is made possible by the larger quantity of available statistics from running much longer, and is justified by the potential for the yield on a single APD gang to vary more rapidly than the yield averaged across an entire APD plane.

It is well-known~\cite{MultivariateDensityScott} that when it is necessary to estimate a multivariate function from limited statistics, a choice of binning can have a significant impact on the result.  It is preferable to use an unbinned technique such as kernel regression.  In particular, our data density is highly non-uniform, and it should be possible to measure the lightmap which high fidelity in regions of high statistics, while smoothly transitioning to a coarser interpolation in regions of low statistics to minimize the impact of uncertainty from individual hits.  State-of-the-art solutions to this problem would rely on locally adaptive kernel regression; see ~\cite{MultivariateDensityScott} for a detailed explanation of the related issues in kernel estimation.  (SHOULD HAVE A CHAPTER ON REGRESSION AS WELL -- BUT UNFORTUNATELY LEFT IT IN THE OFFICE, AND CAN'T ACCESS ONLINE.  RETURN TO THIS.)  Attempting to use a locally adaptive kernel regression for $R(x,y,z)$ should be considered a highly appealing extension to the algorithm for generating a lightmap.

The parametrization of $S(t)$ presents a very different set of choices.  Thorium data is taken in bursts, with the time between mostly filled by low-background runtime.  When we choose to treat $S(t)$ as a smoothly varying function, it becomes critical to interpolate properly -- after all, the low-background runtime is the critical part of the experiment.  (If we only produce a lightmap accurate during source runtime, we will measure an energy resolution from source data which is not borne out in the low-background data, so we should in fact be able to give some guarantee that $S(t)$ is almost as accurate during the low-background runtime as during the source runtime.)  Fortunately, it is generally true that the time variation of the APD response is quite slow; exceptions are generally due to changes in the electronics which should occur at well-specified times.

Currently each source run is treated as a burst of data taken at the midpoint of the run, and $S(t)$ is measured at that point only from the data of that run; then $S(t)$ is linearly interpolated between those points.  In principle it is possible that between two source runs an electronics change may have been performed, which would mean that a better interpolation would be to assume $S(t)$ is flat except for a step at the time of the electronics change; in practice, though, it has generally been the practice of the collaboration to take a source run immediately before and after an electronics change, so no high-value data is taken during that interval and this detail can be ignored.

Another concern with this method is what should be done with short source runs.  If a run is too short, then the measurement of $S(t)$ coming from that run may have significant errors.  We currently mitigate this issue by entirely throwing out all data from runs with fewer than $100$ usable events.  We justify this approach by claiming that even though those events might in principle have contributed some useful information on $R(x,y,z)$, without a good measurement of the relevant $S(t)$ it is impossible to use that data.

It would be better if we made use of our knowledge that $S(t)$ changes slowly or at well-defined moments.  In the future, it would be useful if instead we performed smoothed interpolations between electronics changes.  This could be done in the same style as EXO's lifetime corrections, using polynomial fits, where the degree of the polynomial could be determined by eye.

Alternatively, often there will be a long string of consecutive source runs which should be combined into one higher-statistics dataset.  This process would need to be done essentially by hand, and has not been performed for the current analysis, but it could benefit the accuracy of the $S(t)$ function and also recover some statistics in cases where the individual runs might be too short for inclusion in the lightmap dataset.

The choice of binning or parametrization for $R(x,y,z)$ and $S(t)$ can have a profound impact on the accuracy of the lightmap, and the current analysis has only skimmed the surface in trying options.  It is hoped that further work on the lightmap will include significant investigation in these topics.













